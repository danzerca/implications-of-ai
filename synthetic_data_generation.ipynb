{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-experimental 0.0.64 requires langchain-community<0.3.0,>=0.2.10, but you have langchain-community 0.3.0 which is incompatible.\n",
      "langchain-experimental 0.0.64 requires langchain-core<0.3.0,>=0.2.27, but you have langchain-core 0.3.2 which is incompatible.\n",
      "langgraph 0.2.16 requires langchain-core<0.3,>=0.2.27, but you have langchain-core 0.3.2 which is incompatible.\n",
      "langchain-huggingface 0.0.3 requires langchain-core<0.3,>=0.1.52, but you have langchain-core 0.3.2 which is incompatible.\n",
      "ragas 0.1.20 requires langchain-core<0.3, but you have langchain-core 0.3.2 which is incompatible.\n",
      "langgraph-checkpoint 1.0.6 requires langchain-core<0.3,>=0.2.22, but you have langchain-core 0.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU langsmith langchain-core langchain-community langchain-openai langchain-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pymupdf ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM_Midterm - SDG - {uuid4().hex[0:8]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimakerspace.text_utils import CharacterTextSplitter, PDFFileLoader\n",
    "\n",
    "pdf_loader_NIST = PDFFileLoader(\"data/NIST.AI.600-1.pdf\")\n",
    "pdf_loader_Blueprint = PDFFileLoader(\"data/Blueprint-for-an-AI-Bill-of-Rights.pdf\")\n",
    "documents_NIST = pdf_loader_NIST.load_documents()\n",
    "documents_Blueprint = pdf_loader_Blueprint.load_documents()\n",
    "\n",
    "text_splitter = CharacterTextSplitter()\n",
    "split_documents_NIST = text_splitter.split_texts(documents_NIST)\n",
    "split_documents_Blueprint = text_splitter.split_texts(documents_Blueprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimakerspace.openai_utils.prompts import (\n",
    "    UserRolePrompt,\n",
    "    SystemRolePrompt,\n",
    ")\n",
    "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
    "from aimakerspace.vectordatabase import VectorDatabase\n",
    "\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = \"\"\" \\\n",
    "Use the provided context to answer the user's query.\n",
    "You may not answer the user's query unless there is specific context in the following text.\n",
    "If you do not know the answer, or cannot answer, please respond with \"I don't know\".\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = SystemRolePrompt(RAG_PROMPT_TEMPLATE)\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\" \\\n",
    "Context:\n",
    "{context}\n",
    "User Query:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = UserRolePrompt(USER_PROMPT_TEMPLATE)\n",
    "\n",
    "class RetrievalAugmentedQAPipeline:\n",
    "    def __init__(self, llm: ChatOpenAI(), vector_db_retriever: VectorDatabase) -> None:\n",
    "        self.llm = llm\n",
    "        self.vector_db_retriever = vector_db_retriever\n",
    "\n",
    "    async def arun_pipeline(self, question: str):\n",
    "        context_list = self.vector_db_retriever.search_by_text(question, k=4)\n",
    "\n",
    "        context_prompt = \"\"\n",
    "        for context in context_list:\n",
    "            context_prompt += context[0] + \"\\n\"\n",
    "\n",
    "        formatted_system_prompt = rag_prompt.create_message()\n",
    "\n",
    "        formatted_user_prompt = user_prompt.create_message(question=question, context=context_prompt)\n",
    "\n",
    "        async def generate_response():\n",
    "            async for chunk in self.llm.astream([formatted_system_prompt, formatted_user_prompt]):\n",
    "                yield chunk\n",
    "\n",
    "        return {\"response\": generate_response(), \"context\": context_list}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f529c44a2a44dff8e479212266843ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22970a2c88694a778a84f443ba7fc6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 2, 'score': 1.25}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 0 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Listening to the American People', 'Algorithmic and data-driven harms', 'Panel discussions', 'Consumer rights and protections', 'Automated society']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI Risk Management Framework', 'Trustworthy AI', 'Bias in Artificial Intelligence', 'Language models', 'AI deception']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Deepfake', 'AI Chatbot', 'Large language models', 'Generative AI', 'Information integrity']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated sentiment analyzer', 'Search engine results', 'Advertisement delivery systems', 'Body scanners at airport checkpoints', 'Algorithmic discrimination protections']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated sentiment analyzer', 'Search engine results', 'Advertisement delivery systems', 'Body scanners at airport checkpoints', 'Algorithmic discrimination protections']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Sensitive domains', 'Human oversight', 'Meaningful access', 'Reporting']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Confabulation', 'GAI systems', 'False content', 'Risks of confabulated content', 'Trustworthy AI Characteristics']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Equitable outcomes', 'Timely consideration', 'Effective organizational structure', 'Training and assessment']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 3, 'structure': 3, 'relevance': 3, 'score': 3.0}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Data privacy', 'Automated systems', 'Privacy by design', 'Data collection', 'Risk identification and mitigation']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI Risk Management Framework', 'Trustworthy AI', 'Bias in Artificial Intelligence', 'Language models', 'AI deception']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 0 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Sensitive domains', 'Human oversight', 'Meaningful access', 'Reporting']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Algorithmic discrimination', 'Automated systems', 'Protected classifications', 'Equitable design', 'Disparity testing']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Predictive policing system', 'Gun violence', 'Watch list', 'System flaws', 'Explanation']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How do advertisement delivery systems reinforce racial and gender stereotypes?\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 3, 'score': 1.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Deepfake', 'AI Chatbot', 'Large language models', 'Generative AI', 'Information integrity']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What are the potential risks associated with the use of deepfake technology according to the sources provided?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What measures should be taken to limit data collection in automated systems according to the provided context?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What information should be included in reporting for automated systems used in sensitive domains?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What were the opportunities and challenges discussed in the panel on consumer rights and protections in the context of AI-enabled products and services?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What are some examples of algorithmic discrimination protections being implemented to address bias in technology platforms?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How should the organizational structure surrounding processes for consideration and fallback be designed to ensure effectiveness in automated systems?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are the risks associated with confabulated content in GAI systems, particularly in healthcare applications?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What resources are provided in the National Institute of Standards and Technology's AI Risk Management Framework for understanding AI risks?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What role does human oversight play in ensuring the ethical use of automated systems within sensitive domains?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What resources are available for understanding the language of Trustworthy AI according to the National Institute of Standards and Technology?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How can designers, developers, and deployers of automated systems protect individuals and communities from algorithmic discrimination?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"Why is it important for both police and the public to understand why and how individuals are placed on a watch list?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Notice and explanations', 'American public', 'Automated decision-making processes', 'Explanatory systems']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 0 times\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What are the potential risks associated with the use of deepfake technology, as discussed in the articles referenced in the given context?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"Why is providing notice and explanation important in the context of automated decision-making processes?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for examples of algorithmic discrimination protections aimed at addressing bias in technology platforms. It is clear in its intent, specifying the topic of interest (algorithmic discrimination protections) and the context (technology platforms). The question is self-contained and does not rely on external references, making it understandable and answerable based on the details provided. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What are some examples of algorithmic discrimination protections being implemented to address bias in technology platforms?\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Data privacy', 'Privacy Act of 1974', 'NIST Privacy Framework', 'Biometrics moratorium', 'Workplace surveillance']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question clearly asks about the risks associated with confabulated content in Generative AI (GAI) systems, specifically within the context of healthcare applications. It is specific and independent, as it does not rely on external references or additional context to be understood. The intent is clear, seeking information on potential risks. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What are the risks associated with confabulated content in GAI systems, particularly in healthcare applications?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is clear and specific, asking about the resources available in the National Institute of Standards and Technology's AI Risk Management Framework related to understanding AI risks. It does not rely on external references or additional context, making it self-contained. The intent is straightforward, seeking information about specific resources. Therefore, it meets the criteria for clarity and answerability.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the impact of advertisement delivery systems on reinforcing racial and gender stereotypes. It is specific and has a clear intent, focusing on the relationship between advertisement systems and societal stereotypes. However, it could benefit from additional context or clarification regarding which advertisement delivery systems are being referred to (e.g., digital, traditional media) and what specific aspects of stereotypes are being considered (e.g., portrayal, targeting). Providing such details would enhance the clarity and answerability of the question.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question addresses the roles of designers, developers, and deployers of automated systems in protecting individuals and communities from algorithmic discrimination. It is clear in its intent, specifying the target audience (designers, developers, deployers) and the issue at hand (algorithmic discrimination). However, the question is somewhat broad and could benefit from more specificity regarding the types of automated systems or contexts being referred to (e.g., AI in hiring, law enforcement, etc.). To improve clarity and answerability, the question could specify particular scenarios or examples of algorithmic discrimination, or ask for specific strategies or frameworks that could be employed to mitigate such discrimination.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What measures have been taken to combat bias in technology platforms through algorithmic discrimination protections?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the design of an organizational structure related to processes for consideration and fallback in automated systems. It is somewhat clear in its intent, focusing on effectiveness in automation. However, the phrasing is somewhat vague, particularly regarding what is meant by 'consideration and fallback' processes. Additionally, the term 'organizational structure' could refer to various aspects, such as hierarchy, roles, or workflows, which may lead to ambiguity in the response. To improve clarity and answerability, the question could specify what aspects of the organizational structure are being referred to (e.g., roles, responsibilities, communication channels) and provide examples of what 'consideration and fallback' entails in the context of automated systems.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"How should the organizational structure surrounding processes for consideration and fallback be designed to ensure effectiveness in automated systems?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the potential risks associated with deepfake technology, referencing 'the sources provided' without including any specific information or context about these sources. This reliance on unspecified external references makes the question unclear and not fully self-contained. To improve clarity and answerability, the question could be reframed to either summarize the key points from the sources or to ask for a general overview of the risks without depending on specific documents. For example, it could be rephrased as 'What are the general potential risks associated with the use of deepfake technology?' which would make it more accessible.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: What are the potential risks associated with the use of deepfake technology according to the sources provided?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the importance of understanding the criteria and processes behind placing individuals on a watch list, specifically for both police and the public. It is clear in its intent and does not rely on external references, making it self-contained. However, it could be improved by specifying what aspects of the understanding are considered important (e.g., legal implications, public safety, transparency) to provide a more focused context for the answer. Overall, the question is specific, independent, and has a clear intent.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What sparked the state-wide biometrics moratorium in response to a school board's attempt to surveil public school students in Lockport, New York?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about resources for understanding the language of Trustworthy AI as defined by the National Institute of Standards and Technology (NIST). It is specific in its focus on Trustworthy AI and the NIST, making the intent clear. However, it assumes familiarity with NIST's work and the specific terminology used in the context of Trustworthy AI without providing any context or examples. To improve clarity and answerability, the question could specify what aspects of Trustworthy AI are of interest (e.g., definitions, guidelines, frameworks) or mention any specific documents or publications from NIST that are relevant. This would help ensure that the question is self-contained and understandable for those who may not be familiar with NIST's resources.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: What resources are available for understanding the language of Trustworthy AI according to the National Institute of Standards and Technology?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about measures to limit data collection in automated systems, referencing 'the provided context' without including or describing this context within the question itself. This reliance on unspecified context makes the question unclear and potentially unanswerable for those who do not have access to that context. To improve clarity and answerability, the question should either include the relevant context directly or be rephrased to focus on general measures that can be taken to limit data collection in automated systems, without depending on external references. For example, it could ask, 'What general measures can be implemented to limit data collection in automated systems?'\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What measures should be taken to limit data collection in automated systems according to the provided context?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the role of human oversight in the ethical use of automated systems in sensitive domains. It is clear in its intent, specifying the focus on human oversight and ethical considerations within automated systems. The question is independent and can be understood without needing additional context or references. However, it could be improved by specifying what is meant by 'sensitive domains' (e.g., healthcare, finance, law enforcement) to provide a more focused context for the answer. This would enhance clarity and allow for a more tailored response.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What role does human oversight play in ensuring the ethical use of automated systems within sensitive domains?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the information that should be included in reporting for automated systems in sensitive domains. It is clear in its intent and specifies the context (automated systems in sensitive domains), making it understandable. However, the term 'sensitive domains' could be interpreted in various ways (e.g., healthcare, finance, security), which may lead to ambiguity in the answer. To improve clarity and answerability, the question could specify which sensitive domains are being referred to or provide examples of the types of automated systems in question.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the potential risks of deepfake technology, referencing 'the articles referenced in the given context' without providing any specific details about these articles or the context itself. This reliance on unspecified external references makes the question unclear and unanswerable for those who do not have access to the mentioned articles. To improve clarity and answerability, the question could be reframed to either summarize the key points from the articles or to ask about the risks in a more general sense, without depending on specific external sources. For example, it could ask, 'What are the commonly recognized risks associated with deepfake technology?' which would make it more self-contained.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: What are the potential risks associated with the use of deepfake technology, as discussed in the articles referenced in the given context?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What potential dangers could arise from the presence of confabulated content in GAI systems, especially in healthcare scenarios, and how might this relate to the generation of harmful or violent content by GAI systems?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the importance of providing notice and explanation in automated decision-making processes. It is clear in its intent, specifying the context (automated decision-making processes) and seeking an explanation of the significance of notice and explanation. The question is independent and does not rely on external references or additional context, making it understandable and answerable based on the details provided. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the opportunities and challenges discussed in a specific panel regarding consumer rights and protections related to AI-enabled products and services. While it specifies the topic of interest, it assumes familiarity with the panel's content without providing any context or details about the panel itself. This reliance on external knowledge makes the question less clear and answerable for those who may not have access to the panel's discussions. To improve clarity and answerability, the question could include a brief description of the panel, such as its participants, the date it took place, or key themes that were addressed. This would help provide the necessary context for a more informed response.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What were the opportunities and challenges discussed in the panel on consumer rights and protections in the context of AI-enabled products and services?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What mechanisms should be in place to ensure ethical use of automated systems in sensitive domains, considering both human oversight and alternatives?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"The National Institute of Standards and Technology's AI Risk Management Framework provides resources such as the AI RMF Playbook, Framing Risk information, and The Language of Trustworthy AI glossary to help understand AI risks.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Advertisement delivery systems reinforce racial and gender stereotypes by predicting who is most likely to click on a job advertisement and delivering ads in ways that perpetuate biases. For example, these systems may overwhelmingly direct supermarket cashier ads to women and jobs with taxi companies to primarily Black people, thus perpetuating stereotypes related to gender and race.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Both police and the public deserve to understand why and how individuals are placed on a watch list to ensure transparency and accountability in the decision-making process. This understanding helps in building trust, identifying potential biases or errors in the system, and allows for informed discussions on the implications of such actions.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the potential risks associated with deepfake technology, referencing 'the sources provided' without including any specific information or context about these sources. This reliance on unspecified external references makes the question unclear and not fully self-contained. To improve clarity and answerability, the question could be reframed to either summarize the key points from the sources or to ask for a general overview of the risks associated with deepfake technology without depending on specific sources. For example, it could be rephrased as 'What are the general potential risks associated with the use of deepfake technology?'\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Designers, developers, and deployers of automated systems can protect individuals and communities from algorithmic discrimination by taking proactive and continuous measures. These measures include conducting proactive equity assessments as part of system design, using representative data, protecting against proxies for demographic features, ensuring accessibility for people with disabilities in design and development, conducting pre-deployment and ongoing disparity testing and mitigation, and maintaining clear organizational oversight. Independent evaluation and plain language reporting in the form of an algorithmic impact assessment, including disparity testing results and mitigation information, should be performed and made public whenever possible to confirm these protections.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI Risk Management Framework', 'Trustworthy AI', 'Bias in Artificial Intelligence', 'Language models', 'AI deception']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Algorithmic discrimination', 'Automated systems', 'Protected classifications', 'Equitable design', 'Disparity testing']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Algorithmic discrimination', 'Automated systems', 'Protected classifications', 'Equitable design', 'Disparity testing']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the potential risks of deepfake technology, referencing 'the articles referenced in the given context' without providing any specific details about these articles or the context itself. This reliance on unspecified external references makes the question unclear and unanswerable for those who do not have access to the mentioned articles. To improve clarity and answerability, the question could be reframed to either summarize the key points from the articles or to ask about general risks associated with deepfake technology without depending on specific external sources. For example, it could be rephrased as, 'What are the general potential risks associated with the use of deepfake technology?'\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Data broker exposes', 'Facial recognition technology', 'Surveillance technology in public housing', 'Enforcement actions by the FTC', 'Cheating-detection companies']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How can automated systems contribute to algorithmic discrimination and what measures can be taken to prevent it?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the importance of addressing bias in artificial intelligence according to the National Institute of Standards and Technology?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about measures to limit data collection in automated systems, referencing 'the provided context' without including or describing this context within the question itself. This reliance on unspecified external information makes the question unclear and potentially unanswerable for those who do not have access to that context. To improve clarity and answerability, the question should either include the relevant context directly or be rephrased to focus on general measures that can be taken to limit data collection in automated systems, without depending on external references.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the importance of conducting disparity testing in the context of algorithmic discrimination?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and seeks to understand the events or factors that led to a state-wide biometrics moratorium in relation to a particular incident involving a school board in Lockport, New York. It is clear in its intent, asking for the reasons behind the moratorium and the context of student surveillance. However, it may require some background knowledge about the specific incident and the broader implications of biometrics in schools, which could limit its answerability for those unfamiliar with the topic. To enhance clarity and answerability, the question could briefly outline the key events or concerns that led to the moratorium, or specify the timeframe of the events in question.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Providing notice and explanations in the context of automated decision-making processes is important because it allows individuals to understand how automated systems are impacting their lives. Without clear explanations, people may not know why certain decisions are made, leading to a lack of transparency and accountability. Notice and explanations also help experts verify the reasonableness of recommendations before they are enacted, ensuring safety and efficacy. In order to guard against potential harms, it is crucial for the public to know if automated systems are being used and how they are making decisions that affect rights, opportunities, and access. Clear and valid explanations should be recognized as a baseline requirement to build trust and confidence in the use of automated systems.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI Risk Management Framework', 'Trustworthy AI', 'Bias in Artificial Intelligence', 'Language models', 'AI deception']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about mechanisms for ensuring the ethical use of automated systems in sensitive domains, specifically mentioning human oversight and alternatives. It is clear in its intent and specifies the focus on ethical considerations, making it understandable. However, the question could benefit from being more specific about which sensitive domains it refers to (e.g., healthcare, finance, law enforcement) and what types of automated systems are being considered. This additional detail would help narrow down the scope and provide a more focused answer. Overall, the question is fairly clear but could be improved by specifying the context further.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the design of an organizational structure related to processes for consideration and fallback in automated systems. It is somewhat clear in its intent, focusing on effectiveness in automation. However, the phrasing is somewhat vague, particularly regarding what is meant by 'consideration and fallback' processes. Additionally, the term 'organizational structure' could refer to various aspects, such as hierarchy, roles, or workflows, which may lead to ambiguity in the response. To improve clarity and answerability, the question could specify what aspects of the organizational structure are being referred to (e.g., roles, responsibilities, communication channels) and provide examples of what 'consideration and fallback' entails in the context of automated systems.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How has facial recognition technology been used in public housing and what backlash has it prompted?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question addresses the potential dangers of confabulated content in Generative AI (GAI) systems, particularly in healthcare contexts, and its relation to the generation of harmful or violent content. It is specific in its focus on healthcare scenarios and the implications of confabulated content. However, the question could be clearer by breaking it into two distinct parts: one focusing on the dangers of confabulated content in healthcare and another on the relationship to harmful or violent content. This would help clarify the intent and make it easier to provide a structured response. Additionally, defining 'confabulated content' within the question could enhance understanding for those unfamiliar with the term.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about resources for understanding the language of Trustworthy AI as defined by the National Institute of Standards and Technology (NIST). It is specific in its focus on Trustworthy AI and the NIST, which provides a clear intent. However, it assumes familiarity with NIST's work and the specific terminology used in the context of Trustworthy AI without providing any context or examples. To improve clarity and answerability, the question could specify what aspects of Trustworthy AI are of interest (e.g., definitions, guidelines, frameworks) or mention any specific documents or publications from NIST that are relevant. This would help ensure that the question is more self-contained and accessible to a broader audience.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about measures taken to combat bias in technology platforms specifically through algorithmic discrimination protections. It is clear in its intent, focusing on a specific issue (bias in technology platforms) and the type of measures (algorithmic discrimination protections). However, the question could benefit from being more specific about which technology platforms or types of bias it refers to, as this could lead to a more focused and relevant answer. To improve clarity and answerability, the question could specify particular platforms (e.g., social media, search engines) or types of bias (e.g., racial, gender) that are of interest.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the opportunities and challenges discussed in a specific panel regarding consumer rights and protections related to AI-enabled products and services. While it specifies the topic of interest, it assumes familiarity with the panel's content without providing any context or details about the panel itself. This reliance on external knowledge makes the question less clear and answerable for those who may not have access to the panel's discussions. To improve clarity and answerability, the question could include a brief description of the panel, such as who participated, when it took place, or key themes that were addressed. This would help provide the necessary context for a more informed response.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What mechanisms are needed for ethical use of automated systems in sensitive domains, including human oversight and alternatives?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How can bias in artificial intelligence be identified and managed according to the National Institute of Standards and Technology?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI system context', 'Harmful Bias and Homogenization', 'Risk measurement plans', 'Human-AI Configuration', 'Dangerous, Violent, or Hateful Content']\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What risks come from confabulated content in GAI systems, especially in healthcare, and its link to harmful content generation?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What steps have been taken to address bias in tech platforms with algorithmic discrimination protections?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Reporting for automated systems used in sensitive domains should include an assessment of timeliness and the extent of additional burden for human alternatives, aggregate statistics about who chooses the human alternative, along with the results of the assessment about brevity, clarity, and accessibility of notice and opt-out instructions. Reporting on the accessibility, timeliness, and effectiveness of human consideration and fallback should be made public at regular intervals for as long as the system is in use. This should include aggregated information about the number and type of requests for consideration, fallback employed, and any repeated requests; the timeliness of the handling of these requests, including mean wait times for different types of requests as well as maximum wait times; and information about the procedures used to address requests for consideration along with the results of the evaluation of their accessibility. For systems used in sensitive domains, reporting should include information about training and governance procedures for these technologies. Reporting should also include documentation of goals and assessment of meeting those goals, consideration of data included, and documentation of the governance of reasonable access to the technology. Reporting should be provided in a clear and machine-readable manner.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Continuous monitoring', 'GAI system impacts', 'Structured feedback mechanisms', 'Harmful Bias and Homogenization', 'Information Integrity']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI Bill of Rights', 'White House Office of Science and Technology Policy', 'Automated Systems', 'Civil Rights', 'Democratic Values']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking about the contribution of automated systems to algorithmic discrimination and the measures that can be taken to prevent it. It does not rely on external references and can be understood independently. The intent is also clear, as it seeks both an explanation of the problem and potential solutions. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"How can automated systems contribute to algorithmic discrimination and what measures can be taken to prevent it?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Algorithmic discrimination', 'Independent evaluation', 'Algorithmic impact assessment', 'Reporting']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What factors should be assessed when determining the expected and acceptable GAI system context of use?\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What should be included in an algorithmic impact assessment for automated systems?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses specifically on the role of human oversight in ethical use, while the second question expands the inquiry to include mechanisms for ethical use and alternatives, indicating a broader scope and depth.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How can continuous monitoring be implemented to identify whether GAI system outputs are equitable across various sub-populations?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How can automated systems contribute to algorithmic discrimination and what protective measures should be implemented to prevent it?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the importance of conducting disparity testing specifically in the context of algorithmic discrimination. It is clear in its intent, focusing on a specific topic (disparity testing) and its relevance to a particular issue (algorithmic discrimination). The question is independent and does not rely on external references or additional context, making it understandable and answerable based on the details provided. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: What is the importance of conducting disparity testing in the context of algorithmic discrimination?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the role of democratic values in the building, deployment, and governance of automated systems according to the Blueprint for an AI Bill of Rights?\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the risks of confabulated content in GAI systems within healthcare applications. However, the second question introduces an additional aspect regarding harmful content generation, which expands the breadth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about measures taken to combat bias in technology platforms, specifically focusing on algorithmic discrimination protections. They share the same constraints and requirements, as well as similar depth and breadth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the use of facial recognition technology in public housing and the backlash it has prompted. It is clear in its intent, specifying both the application (public housing) and the consequences (backlash). The question is independent and can be understood without needing additional context or external references. However, to enhance clarity, it could specify the type of backlash (e.g., legal, social, ethical) or provide examples of how the technology has been used in public housing. This would allow for a more focused and detailed response.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: How has facial recognition technology been used in public housing and what backlash has it prompted?\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"A state-wide biometrics moratorium was sparked by a school board's attempt to surveil public school students in Lockport, New York, leading to the banning of facial recognition systems and other biometric identifying technology in schools until July 1, 2022. The law also requires a report on the privacy, civil rights, and civil liberties implications of such technologies before their use in New York schools.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI-generated content', 'Real-time auditing tools', 'Structured feedback mechanisms', 'Synthetic data', 'Incident response and recovery plans']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the importance of addressing bias in artificial intelligence as per the National Institute of Standards and Technology (NIST). It is clear in its intent, specifying both the topic (bias in AI) and the source of information (NIST). However, it assumes familiarity with NIST's publications or guidelines on this topic without providing any context or details. To improve clarity and answerability, the question could specify which NIST document or report it refers to, or summarize the key points of NIST's stance on bias in AI. This would help ensure that the question is self-contained and understandable for those who may not have direct access to NIST's materials.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: What is the importance of addressing bias in artificial intelligence according to the National Institute of Standards and Technology?\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Healthcare navigators', 'Automated customer service', 'Ballot curing laws', 'AI-driven call response systems', 'Fallback system']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is clear and specific, asking about the identification and management of bias in artificial intelligence as per the guidelines or recommendations from the National Institute of Standards and Technology (NIST). It does not rely on external references beyond the mention of NIST, which is a well-known organization. However, it could be improved by specifying whether the question seeks a summary of NIST's guidelines, specific methodologies, or examples of bias management practices. This would help narrow down the expected response and enhance clarity.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How can synthetic data be responsibly used in GAI development to match the statistical properties of real-world data without disclosing personally identifiable information or contributing to homogenization?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How do ballot curing laws provide a fallback system for voters to correct their ballots in case of issues?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question addresses the role of automated systems in contributing to algorithmic discrimination and seeks information on protective measures to prevent such discrimination. It is clear in its intent, specifying two distinct areas of inquiry: the contribution of automated systems to algorithmic discrimination and the protective measures that can be implemented. The question is independent and can be understood without needing additional context or references. However, it could be improved by specifying the types of automated systems or contexts in which this discrimination occurs, as well as the nature of the protective measures being sought (e.g., legal, technical, ethical). This would enhance clarity and focus.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the role of disparity testing in safeguarding individuals and communities against algorithmic discrimination. It is specific and has a clear intent, focusing on the relationship between disparity testing and its protective effects. The question can be understood and answered without needing additional context or external references, making it self-contained. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The expectations for automated systems in sensitive domains include additional human oversight and safeguards, narrowly scoped data and inferences, tailored systems, human consideration before high-risk decisions, meaningful access to examine the system, access to human alternatives, timely human consideration and remedy by a fallback system, proportionate availability of human consideration and fallback, accessible mechanisms for human consideration and fallback, and convenient opt-out mechanisms.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the components that should be included in an algorithmic impact assessment for automated systems. It is clear in its intent, specifying the focus on 'algorithmic impact assessment' and 'automated systems'. However, it could benefit from being more specific about the context or criteria for the assessment, as the term 'algorithmic impact assessment' can encompass various aspects such as ethical considerations, performance metrics, or regulatory compliance. To improve clarity and answerability, the question could specify whether it is seeking a general overview or particular elements related to a specific industry or application.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What should be included in an algorithmic impact assessment for automated systems?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the role of democratic values in the context of automated systems as outlined in the 'Blueprint for an AI Bill of Rights'. It specifies the topic (democratic values) and the context (automated systems and the AI Bill of Rights), making the intent clear. However, it assumes familiarity with the 'Blueprint for an AI Bill of Rights' without providing any details about it, which could hinder understanding for those not acquainted with the document. To improve clarity and answerability, the question could briefly summarize what the 'Blueprint for an AI Bill of Rights' entails or specify which aspects of democratic values are being referred to (e.g., fairness, accountability, transparency).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: What is the role of democratic values in the building, deployment, and governance of automated systems according to the Blueprint for an AI Bill of Rights?\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"Risks from confabulated content in GAI systems, especially in healthcare, include the potential for doctors to make incorrect diagnoses or recommend the wrong treatments based on false information. This can lead to harmful consequences for patients. Additionally, confabulated logic or citations in GAI outputs may mislead humans into inappropriately trusting the system's answers, even when they are incorrect. The link to harmful content generation arises from the fact that GAI systems can produce content that is inciting, radicalizing, or threatening, with greater ease and scale than other technologies, potentially leading to real-world harm.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: How does conducting disparity testing contribute to protecting individuals and communities from algorithmic discrimination?\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Towards a Standard for Identifying and Managing Bias in Artificial Intelligence', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How can automated systems contribute to algorithmic discrimination and what protective measures should be taken?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What components should be part of an algorithmic impact assessment for automated systems to ensure fairness and accessibility?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the implementation of continuous monitoring to assess the equity of GAI system outputs across different sub-populations. It is specific in its focus on continuous monitoring and the concept of equity in outputs, making the intent clear. However, the question could benefit from additional context regarding what is meant by 'equitable outputs' and what specific sub-populations are being referred to. To improve clarity and answerability, the question could specify the criteria for equity, the types of GAI systems in question, or examples of sub-populations to provide a clearer framework for the response.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the factors to consider when determining the expected and acceptable context of use for a General Artificial Intelligence (GAI) system. It is clear in its intent, specifying the subject matter (GAI system) and the focus on factors for assessment. However, the term 'context of use' could be interpreted in various ways, and the question does not provide specific parameters or examples of what is meant by 'expected' and 'acceptable'. To improve clarity and answerability, the question could specify what aspects of the context of use are being referred to (e.g., ethical considerations, operational environments, user interactions) or provide examples of the types of factors that might be relevant.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What factors should be assessed when determining the expected and acceptable GAI system context of use?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the importance of addressing bias in artificial intelligence as per the National Institute of Standards and Technology (NIST). It is clear in its intent, specifying both the topic (bias in AI) and the source of information (NIST). However, it assumes familiarity with NIST's publications or guidelines without providing any context or details about what those might entail. To improve clarity and answerability, the question could specify which NIST document or report it refers to, or summarize the key points related to bias in AI that are relevant to the inquiry.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the backlash against the use of facial recognition technology in public housing, specifically referencing reports by the New York Times. While it is clear in its intent to understand the reasons for the backlash, it relies on specific reporting from a particular source (the New York Times) without providing any context or details from that source. This makes it less independent and potentially unclear for those who may not have access to or knowledge of the specific articles. To improve clarity and answerability, the question could either summarize the key points from the New York Times reports or ask for general reasons for the backlash without referencing a specific source.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"How has the use of facial recognition technology in public housing led to backlash, as reported by the New York Times?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions address the role of disparity testing in relation to algorithmic discrimination, focusing on its importance and preventive measures, thus sharing the same constraints and depth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] evolution_filter failed, retrying with 1\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is specific and seeks to explore the responsible use of synthetic data in Generative AI (GAI) development, focusing on matching statistical properties of real-world data while ensuring privacy and avoiding homogenization. It is clear in its intent and does not rely on external references, making it understandable and answerable based on the details provided. However, it could be improved by breaking it down into more manageable parts or clarifying what is meant by 'homogenization' in this context, as this term can have various interpretations. Overall, the question is well-structured and meets the criteria for clarity and answerability.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"How can synthetic data be responsibly used in GAI development to match the statistical properties of real-world data without disclosing personally identifiable information or contributing to homogenization?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What considerations are essential in determining the anticipated and acceptable GAI system context of use, taking into account factors such as socio-cultural collaboration, risk measurement plans, interdisciplinary teams, continuous monitoring of impacts, and structured human feedback exercises?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI technologies', 'Content Provenance', 'Provenance data tracking', 'Synthetic content detection', 'Digital transparency mechanisms']\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': \"Both questions inquire about the role of automated systems in algorithmic discrimination and seek information on measures to address it. The terms 'prevent' and 'protective measures' imply similar requirements and depth of inquiry.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about how ballot curing laws serve as a fallback system for voters to correct their ballots when issues arise. It is specific in its focus on ballot curing laws and their function, making the intent clear. The question is independent as it does not rely on external references or context to be understood. Therefore, it is understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"How do ballot curing laws provide a fallback system for voters to correct their ballots in case of issues?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Continuous monitoring of GAI system impacts can be implemented by seeking active and direct feedback from affected communities through structured feedback mechanisms or red-teaming. This approach helps to monitor and improve outputs to ensure equity across various sub-populations.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the role of democratic values in the context of automated systems as outlined in the 'Blueprint for an AI Bill of Rights'. It specifies the topic (democratic values) and the context (automated systems and the AI Bill of Rights), making the intent clear. However, it assumes familiarity with the 'Blueprint for an AI Bill of Rights' without providing any details about it, which could hinder understanding for those not acquainted with the document. To improve clarity and answerability, the question could briefly summarize what the 'Blueprint for an AI Bill of Rights' entails or specify which aspects of democratic values are being referred to (e.g., fairness, accountability, transparency).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How can digital transparency mechanisms like provenance data tracking help manage and mitigate risks associated with GAI technologies?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Synthetic training data', 'Non-synthetic training data', 'Model collapse', 'Environmental impact', 'Sustainability of AI model training']\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How can synthetic data be responsibly used in GAI development to match the statistical properties of real-world data without disclosing personally identifiable information or contributing to homogenization while ensuring incident response and recovery plans are updated and GAI systems are deactivated when necessary?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking about the components necessary for an algorithmic impact assessment focused on fairness and accessibility in automated systems. It does not rely on external references or context, making it independent and understandable. The intent is evident, as it seeks a list or description of components relevant to the assessment. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 3, 'structure': 3, 'relevance': 3, 'score': 3.0}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Abusive data practices', 'Privacy protections', 'Data collection', 'Consent requests', 'Surveillance technologies']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How is the environmental impact and sustainability of AI model training and management activities assessed and documented?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How do ballot curing laws ensure that voters have a mechanism to rectify ballot issues, considering the importance of human alternatives and fallback systems in safeguarding rights and access?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What's needed for a fair and accessible algorithmic impact assessment for automated systems?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Ongoing monitoring', 'Periodic review', 'Organizational roles and responsibilities', 'GAI systems', 'AI system inventory']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How should individuals be protected from abusive data practices according to the given context?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the backlash against the use of facial recognition technology in public housing, specifically referencing reports by the New York Times. While it is clear in its intent to understand the reasons for the backlash, it relies on specific reporting from a particular source (the New York Times) without providing any context or details from that source. This makes it less independent and potentially unclear for those who may not have access to or knowledge of the specific articles referenced. To improve clarity and answerability, the question could either summarize the key points from the New York Times reports or ask for general reasons for backlash against facial recognition technology in public housing without tying it to a specific source.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What mechanisms are in place for inventorying AI systems and how are they resourced according to organizational risk priorities?\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions focus on the requirements for an algorithmic impact assessment for automated systems, but the first question is more general while the second emphasizes fairness and accessibility, indicating a difference in depth and breadth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking about the role of digital transparency mechanisms, specifically provenance data tracking, in managing and mitigating risks related to Generative AI (GAI) technologies. It does not rely on external references and can be understood independently. The intent is clear, seeking an explanation of how these mechanisms function in the context of risk management. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"How can digital transparency mechanisms like provenance data tracking help manage and mitigate risks associated with GAI technologies?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Biometric Information Privacy Act', 'Fair Credit Reporting Act', 'Explainable AI systems', 'California law', 'Automated technologies']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is quite complex and covers multiple factors related to the context of use for GAI systems. While it specifies various considerations (socio-cultural collaboration, risk measurement plans, etc.), the phrasing is somewhat convoluted, which may lead to ambiguity in understanding the core intent. Additionally, the question assumes familiarity with terms like 'GAI system' and 'structured human feedback exercises' without providing definitions or context. To improve clarity and answerability, the question could be simplified and broken down into more specific sub-questions or rephrased to focus on one or two key considerations at a time. For example, it could ask, 'What are the key socio-cultural factors to consider when determining the context of use for GAI systems?' This would make it more approachable and easier to answer.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What considerations are essential in determining the anticipated and acceptable GAI system context of use, taking into account factors such as socio-cultural collaboration, risk measurement plans, interdisciplinary teams, continuous monitoring of impacts, and structured human feedback exercises?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the assessment and documentation of the environmental impact and sustainability of AI model training and management activities. It is clear in its intent, specifying the focus on environmental impact and sustainability. However, the question could be improved by clarifying what specific aspects of assessment and documentation are of interest (e.g., metrics used, frameworks, case studies) and whether it seeks general practices or specific examples. This would help narrow down the response and make it more answerable. Overall, while the question is relatively clear, adding more detail about the desired scope would enhance its clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: \"How is the environmental impact and sustainability of AI model training and management activities assessed and documented?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks how individuals should be protected from abusive data practices, referencing 'the given context' without providing any specific details about that context. This reliance on unspecified external information makes the question unclear and unanswerable for those who do not have access to the context. To improve clarity and answerability, the question could either include a brief summary of the relevant context or be rephrased to ask for general strategies or principles for protecting individuals from abusive data practices, independent of any specific context.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: How should individuals be protected from abusive data practices according to the given context?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What are some provisions included in the Biometric Information Privacy Act enacted by the state of Illinois?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and seeks to understand how ballot curing laws provide mechanisms for voters to address ballot issues, while also considering the role of human alternatives and fallback systems in protecting rights and access. It is clear in its intent and does not rely on external references, making it understandable and answerable based on the details provided. However, it could be improved by simplifying the language or breaking it into two parts for clarity, as the current structure may be slightly complex for some audiences. For example, separating the inquiry about ballot curing laws from the discussion on human alternatives could enhance clarity.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is complex and multifaceted, addressing the responsible use of synthetic data in General Artificial Intelligence (GAI) development while considering various ethical and operational aspects. However, it is overly lengthy and convoluted, making it difficult to parse and understand the specific intent. The question combines several distinct issues: the use of synthetic data, the need to match statistical properties of real-world data, the importance of protecting personally identifiable information, the risk of homogenization, and the necessity of incident response and recovery plans. To improve clarity and answerability, the question could be broken down into smaller, more focused questions, each addressing a specific aspect of the topic. For example, one question could focus on the ethical implications of using synthetic data, while another could address operational protocols for GAI systems.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"How can synthetic data be responsibly used in GAI development to match the statistical properties of real-world data without disclosing personally identifiable information or contributing to homogenization while ensuring incident response and recovery plans are updated and GAI systems are deactivated when necessary?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the mechanisms for inventorying AI systems and their resourcing based on organizational risk priorities. It is specific in its focus on AI systems and organizational risk, which provides a clear intent. However, the question may be challenging for those unfamiliar with the specific organizational context or the term 'inventorying AI systems'. To improve clarity and answerability, it could specify what types of mechanisms are being referred to (e.g., software tools, processes) and provide a brief explanation of how organizational risk priorities are determined. This would help ensure that the question is understood by a broader audience.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How do ballot curing laws protect voter rights?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is quite complex and covers multiple factors related to the context of use for GAI systems. While it specifies various considerations (socio-cultural collaboration, risk measurement plans, etc.), the phrasing is somewhat convoluted, which may lead to ambiguity in understanding the primary focus. Additionally, the question is lengthy and may overwhelm the reader, making it less accessible. To improve clarity and answerability, the question could be broken down into simpler components or rephrased to focus on one or two key considerations at a time. For example, it could ask, 'What are the key socio-cultural factors to consider when determining the context of use for GAI systems?' This would make it more straightforward and easier to address.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks how individuals should be protected from abusive data practices, referencing 'the given context' without providing any specific details about that context. This reliance on unspecified external information makes the question unclear and unanswerable for those who do not have access to the context. To improve clarity and answerability, the question could either include a brief summary of the relevant context or be rephrased to ask for general strategies or principles for protecting individuals from abusive data practices, independent of any specific context.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks for specific provisions included in the Biometric Information Privacy Act enacted by the state of Illinois. It is clear in its intent, specifying the law and the type of information sought (provisions). The question is independent and can be understood without needing additional context or external references. However, it could be improved by specifying whether the asker is interested in particular types of provisions (e.g., penalties, consent requirements) or a general overview. This would help narrow down the response to better meet the asker's needs.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: What are some provisions included in the Biometric Information Privacy Act enacted by the state of Illinois?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How can provenance data tracking aid in managing and reducing risks related to GAI technologies, considering its role in content transparency and authenticity?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 3, 'structure': 3, 'relevance': 3, 'score': 3.0}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Data privacy', 'Automated systems', 'Privacy by design', 'Data collection', 'Risk identification and mitigation']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Algorithmic discrimination', 'Independent evaluation', 'Algorithmic impact assessment', 'Reporting']\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the specific mechanism of ballot curing laws as a fallback system for correcting ballots, while the second question addresses the broader concept of protecting voter rights. This indicates a difference in depth and breadth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is complex and multifaceted, addressing the responsible use of synthetic data in General Artificial Intelligence (GAI) development. While it touches on several important aspects, such as matching statistical properties of real-world data, protecting personally identifiable information, preventing homogenization, and ensuring proper incident response, the length and breadth of the question may lead to ambiguity in intent. It is not clear which specific aspect the question seeks to prioritize or explore in depth. To improve clarity and answerability, the question could be broken down into smaller, more focused questions, each addressing a specific concern (e.g., one question about statistical properties, another about incident response). This would allow for more direct and relevant responses.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Mechanisms for inventorying AI systems include enumerating organizational GAI systems, adjusting AI system inventory requirements to address GAI risks, defining inventory exemptions for GAI systems embedded in application software, and including specific information in inventory entries such as data provenance, known issues, human oversight roles, intellectual property considerations, and underlying models. These mechanisms are resourced according to organizational risk priorities to ensure proper governance, risk management, and oversight of AI systems.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What expectations should be met by automated systems in terms of protecting data privacy?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What should be included in an algorithmic impact assessment for automated systems according to the given context?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is quite detailed and specifies the aspects of environmental impacts and sustainability related to AI model training and management activities. It clearly outlines the factors to be considered, such as energy and water consumption, carbon capture, and resource trade-offs. However, the complexity and breadth of the question may make it challenging to answer comprehensively without additional context or specific frameworks for evaluation. To improve clarity and answerability, the question could be broken down into more specific sub-questions or focus on a particular aspect of evaluation, such as 'What metrics are commonly used to assess energy consumption in AI training?' This would allow for more targeted responses.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and seeks to understand the role of provenance data tracking in managing and reducing risks associated with GAI technologies, particularly in terms of content transparency and authenticity. It is clear in its intent and does not rely on external references, making it understandable and answerable based on the details provided. However, to enhance clarity, the question could specify what types of risks are being referred to (e.g., ethical, legal, operational) or provide examples of GAI technologies for context. This would help in framing a more focused response.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 3, 'score': 1.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Toxic Speech', 'Algorithmic Harms', 'AI Incident Documentation Practices', 'Artificial Intelligence-powered drug discovery', 'Ethical and social risks of harm from Language Models']\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"How are the environmental impacts and sustainability of AI model training and management activities evaluated and recorded, considering factors like energy and water consumption, carbon capture, and trade-offs between resources used at inference and training times?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How does provenance data help manage risks in GAI tech, considering its role in content transparency and authenticity?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking about the requirements for private entities in Illinois concerning the collection of biometric information as per the Biometric Information Privacy Act. It does not rely on external references and conveys a clear intent to understand legal obligations. Therefore, it is understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Ballot curing laws in at least 24 states provide a fallback system that allows voters to correct their ballot if it is flagged as invalid due to issues such as a voter signature mismatch. This process ensures that voters have the opportunity to rectify any errors and have their votes counted, thereby protecting their rights to participate in the electoral process.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the importance of exploring the state of AI incident documentation practices?\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: What specific requirement must be met by private entities in Illinois regarding the collection of biometric information, as outlined in the Biometric Information Privacy Act?\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks about the assessment and documentation of environmental impact and sustainability, while the second question specifies additional factors to consider in the assessment. This leads to a difference in depth and breadth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the role of provenance data in managing risks associated with GAI technologies, focusing on transparency and authenticity. They share similar constraints and depth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks what should be included in an algorithmic impact assessment for automated systems, referencing 'the given context' without providing any details about this context. This reliance on unspecified external information makes the question unclear and unanswerable for those who do not have access to that context. To improve clarity and answerability, the question could either include a brief description of the relevant context or specify the aspects of the algorithmic impact assessment that are of interest (e.g., ethical considerations, performance metrics, stakeholder impacts).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What should be included in an algorithmic impact assessment for automated systems according to the given context?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the expectations for automated systems regarding data privacy protection. It is clear and specific, indicating a focus on the standards or requirements that such systems should meet. The intent is unambiguous, seeking information on data privacy expectations. However, to enhance clarity and answerability, the question could specify the context in which these expectations are being considered (e.g., legal frameworks, industry standards, ethical guidelines) or provide examples of automated systems (e.g., AI, cloud services) to narrow down the scope. This would help in providing a more targeted and relevant response.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What expectations should be met by automated systems in terms of protecting data privacy?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks for a general overview of provisions in the Biometric Information Privacy Act, while the second question focuses specifically on the requirements for private entities regarding biometric information collection. This difference in focus leads to a disparity in depth and breadth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Ongoing monitoring', 'Periodic review', 'Organizational roles and responsibilities', 'GAI systems', 'AI system inventory']\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What standards should automated systems adhere to in order to ensure data privacy protection?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What is the importance of defining organizational roles and responsibilities in the risk management process and periodic review of outcomes?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the importance of exploring AI incident documentation practices, which is a clear and specific inquiry. It does not rely on external references and can be understood independently. However, it could benefit from a bit more specificity regarding what aspects of 'AI incident documentation practices' are of interest (e.g., effectiveness, compliance, learning opportunities). This would help in providing a more focused and relevant answer. Overall, the question is clear and answerable as it stands.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What is the importance of exploring the state of AI incident documentation practices?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks what should be included in an algorithmic impact assessment for automated systems, referencing 'the given context' without providing any details about this context. This reliance on unspecified external information makes the question unclear and unanswerable for those who do not have access to that context. To improve clarity and answerability, the question could either include a brief description of the context or specify the aspects of the algorithmic impact assessment that are of interest (e.g., ethical considerations, performance metrics, stakeholder impacts).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What insights can be gained by examining the current state of AI incident documentation practices?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 3, 'relevance': 3, 'score': 2.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI Actor', 'Suggested actions', 'GAI risks', 'AI RMF functions', 'Legal and regulatory requirements']\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Environmental impacts and sustainability of AI model training and management activities are assessed by measuring or estimating factors such as energy and water consumption for training, fine-tuning, and deploying models. This assessment involves verifying the tradeoffs between resources used at inference time versus additional resources required at training time. Additionally, the effectiveness of carbon capture or offset programs for GAI training and applications is verified to address concerns related to green-washing.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What are some examples of GAI risks mentioned in the given context?\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Private entities in Illinois are required to provide written notice to individuals or their legally appointed representatives before collecting, capturing, purchasing, receiving through trade, or otherwise obtaining biometric information about them, as per the Biometric Information Privacy Act.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the significance of defining organizational roles and responsibilities in the context of risk management and the periodic review of outcomes. It is clear in its intent, focusing on the importance of these definitions within a specific process. The question is self-contained and does not rely on external references, making it understandable and answerable based on the details provided. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What is the importance of defining organizational roles and responsibilities in the risk management process and periodic review of outcomes?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the standards that automated systems should follow to ensure data privacy protection. It is clear in its intent, specifying the focus on standards and the context of data privacy. The question is independent and can be understood without needing additional context or references. However, it could be improved by specifying the type of automated systems (e.g., software, hardware, AI systems) or the context in which these standards apply (e.g., regulatory frameworks, industry best practices). This would help narrow down the scope and provide a more targeted answer.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the insights that can be gained from examining AI incident documentation practices, which is a specific topic. However, it is somewhat vague as it does not specify what aspects of 'current state' or 'insights' are of interest. The term 'current state' could refer to various factors such as trends, challenges, or effectiveness, and 'insights' could encompass a wide range of interpretations. To improve clarity and answerability, the question could specify the particular aspects of AI incident documentation practices being examined (e.g., effectiveness, common challenges, best practices) or the type of insights sought (e.g., operational improvements, compliance issues).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What insights can be gained by examining the current state of AI incident documentation practices?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What benefits arise from clearly defining organizational roles and responsibilities in the risk management process and reviewing outcomes periodically?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What standards must automated systems follow for data privacy?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the requirements and standards related to data privacy in automated systems, sharing the same depth and breadth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the benefits of clearly defining organizational roles and responsibilities in the risk management process and the importance of periodic reviews of outcomes. It is specific and independent, as it does not rely on external references or additional context to be understood. The intent is clear, seeking information on the advantages of these practices in risk management. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks for examples of GAI (General Artificial Intelligence) risks, referencing 'the given context' without providing any specific details about that context. This reliance on unspecified external information makes the question unclear and unanswerable for those who do not have access to the mentioned context. To improve clarity and answerability, the question could either include a brief summary of the relevant context or specify the types of GAI risks being inquired about (e.g., ethical, operational, societal).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: What are some examples of GAI risks mentioned in the given context?\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What advantages come from defining roles and responsibilities in risk management and reviewing outcomes regularly?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 3, 'relevance': 3, 'score': 2.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI Bill of Rights', 'Automated systems', 'Technical companion', 'Principles', 'Protecting rights']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the insights that can be gained from examining AI incident documentation practices, which is a specific topic. However, it is somewhat vague as it does not specify what aspects of 'current state' or 'insights' are of interest. The term 'current state' could refer to various factors such as trends, challenges, or effectiveness, and 'insights' could encompass a wide range of interpretations. To improve clarity and answerability, the question could specify the particular aspects of AI incident documentation practices being examined (e.g., effectiveness, common challenges, best practices) or the type of insights sought (e.g., operational improvements, compliance issues).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What is the purpose of the principles outlined in the Blueprint for an AI Bill of Rights?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the significance and benefits of defining roles and responsibilities in the context of risk management and the review of outcomes, maintaining the same depth and breadth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks for examples of GAI (General Artificial Intelligence) risks, referencing 'the given context' without providing any specific details about that context. This reliance on unspecified external information makes the question unclear and unanswerable for those who do not have access to the context. To improve clarity and answerability, the question could either include a brief summary of the relevant context or specify the types of GAI risks of interest (e.g., ethical, operational, societal).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 3 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Predictive policing system', 'Gun violence', 'Watch list', 'System flaws', 'Explanation']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Algorithmic discrimination', 'Automated systems', 'Protected classifications', 'Equitable design', 'Disparity testing']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How can lack of explanation in a predictive policing system lead to potential problems and lack of transparency?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are the protected classifications that should be safeguarded against algorithmic discrimination according to the given context?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 3 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the purpose of the principles outlined in the Blueprint for an AI Bill of Rights. It is specific and clear in its intent, focusing on the principles and their purpose. However, it assumes familiarity with the 'Blueprint for an AI Bill of Rights' without providing any context or details about what those principles are. To improve clarity and answerability, the question could briefly summarize the principles or specify which aspects of the purpose are of interest (e.g., ethical implications, legal frameworks, societal impact).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What is the purpose of the principles outlined in the Blueprint for an AI Bill of Rights?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['GAI systems', 'Digital content transparency', 'Structured feedback', 'Adversarial testing', 'Interpretability and explainability methods']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the purpose of conducting adversarial testing in the context of GAI systems?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the implications of a lack of explanation in a predictive policing system, specifically regarding potential problems and transparency issues. It is clear in its intent and does not rely on external references, making it understandable and answerable based on the details provided. The question is specific enough to allow for a focused response on the consequences of insufficient explanations in such systems. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How can lack of explanation in a predictive policing system lead to potential problems and lack of transparency?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How does the absence of clarification in a predictive policing system potentially result in issues and a lack of openness?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the purpose of the principles outlined in the Blueprint for an AI Bill of Rights. It is specific and clear in its intent, focusing on the principles and their purpose. However, it assumes familiarity with the 'Blueprint for an AI Bill of Rights' without providing any context or details about what those principles are. To improve clarity and answerability, the question could briefly summarize the principles or specify which aspects of the purpose are of interest (e.g., ethical implications, societal impact).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 3 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about protected classifications that should be safeguarded against algorithmic discrimination, referencing 'the given context' without providing that context within the question itself. This reliance on unspecified external information makes the question unclear and potentially unanswerable for those who do not have access to the context. To improve clarity and answerability, the question could either include a brief description of the relevant context or specify the classifications of interest directly. Additionally, it could clarify whether it seeks legal definitions, ethical considerations, or specific examples of classifications.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"What are the protected classifications that should be safeguarded against algorithmic discrimination according to the given context?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 3 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the potential issues and lack of openness that may arise from the absence of clarification in a predictive policing system. It is specific in its focus on predictive policing and the consequences of a lack of clarification, making the intent clear. However, the phrasing could be improved for clarity. The term 'clarification' is somewhat vague; it could refer to a lack of transparency, unclear algorithms, or insufficient communication with the public. To enhance clarity and answerability, the question could specify what type of clarification is being referred to (e.g., algorithmic transparency, community engagement) and what specific issues are being considered (e.g., ethical concerns, effectiveness).\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the purpose of conducting adversarial testing specifically in the context of Generative AI (GAI) systems. It is clear and specific, indicating the topic of interest (adversarial testing) and its application area (GAI systems). The intent is unambiguous, seeking to understand the rationale behind this testing method. Therefore, the question is independent and can be answered without needing additional context or references. It meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What is the purpose of conducting adversarial testing in the context of GAI systems?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about protected classifications that should be safeguarded against algorithmic discrimination, referencing 'the given context' without providing any specific details about this context. This reliance on unspecified external information makes the question unclear and potentially unanswerable for those who do not have access to the context. To improve clarity and answerability, the question could either include a brief description of the relevant context or specify the classifications of interest. Additionally, it could clarify whether it seeks a general list of protected classifications or specific examples related to a particular domain (e.g., employment, housing, etc.).\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 3 times\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How can lack of clarification in predictive policing lead to problems and lack of transparency?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"Why is it essential to regularly conduct adversarial testing to assess and mitigate risks in GAI systems, considering the need for structured feedback and monitoring of system impacts across various sub-populations?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 3, 'score': 1.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['AI Incident Database', 'Large Language Models', 'Generative AI', 'Ethical Tensions', 'Disinformation Business']\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions address the issues related to the absence of explanation or clarification in predictive policing systems, focusing on potential problems and transparency. They share the same constraints and requirements, as well as depth and breadth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 3 times\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the significance of the AI Incident Database in the field of artificial intelligence research and development?\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Listening to the American People', 'Algorithmic and data-driven harms', 'Panel discussions', 'Consumer rights and protections', 'Automated society']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How did the White House Office of Science and Technology Policy seek input from people across the country on the issue of algorithmic and data-driven harms and potential remedies?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is specific and seeks to understand the importance of regular adversarial testing in Generative AI (GAI) systems, particularly in relation to structured feedback and monitoring impacts on different sub-populations. It is clear in its intent and does not rely on external references, making it understandable. However, the complexity of the terms used (e.g., 'adversarial testing', 'structured feedback', 'sub-populations') may pose a challenge for those unfamiliar with the field. To enhance clarity, the question could be simplified or broken down into more specific components, such as asking about the benefits of adversarial testing in general before linking it to feedback and monitoring.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"Why is regular adversarial testing important for GAI systems to manage risks and monitor impacts on different sub-populations?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking about the actions taken by the White House Office of Science and Technology Policy to gather input on algorithmic and data-driven harms. It does not rely on external references and can be understood independently. The intent is also clear, seeking information about the methods used for public engagement on this issue. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How did the White House Office of Science and Technology Policy seek input from people across the country on the issue of algorithmic and data-driven harms and potential remedies?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How did the OSTP gather input nationwide on algorithmic and data-driven harms for the AI Bill of Rights?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks about the general purpose of adversarial testing in GAI systems, while the second question specifically addresses the importance of this testing in managing risks and monitoring impacts on sub-populations. This leads to different depths and focuses in their inquiries.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the significance of the AI Incident Database in artificial intelligence research and development. It is clear in its intent, specifying the subject (AI Incident Database) and the context (artificial intelligence research and development). The question is independent and does not rely on external references, making it understandable and answerable based on the details provided. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What is the significance of the AI Incident Database in the field of artificial intelligence research and development?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What role does the AI Incident Database play in enhancing the safety and security of artificial intelligence systems?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Regular adversarial testing is important for GAI systems to manage risks and monitor impacts on different sub-populations because it helps map and measure GAI risks, including attempts to deceive or manipulate provenance techniques or other misuses. By conducting adversarial testing, vulnerabilities can be identified, potential misuse scenarios understood, and unintended outputs revealed, contributing to the overall trustworthiness and reliability of the system across various sub-populations.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the methods used by the OSTP (Office of Science and Technology Policy) to gather nationwide input on algorithmic and data-driven harms related to the AI Bill of Rights. It is specific and clear in its intent, focusing on a particular organization and a defined topic. However, it assumes familiarity with the OSTP and the AI Bill of Rights without providing any context about what these entail. To improve clarity and answerability for a broader audience, the question could include a brief explanation of the OSTP's role and the significance of the AI Bill of Rights, or specify the types of input being referred to (e.g., public comments, expert consultations).\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How did the OSTP collect input for the AI Bill of Rights?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question clearly asks about the role of the AI Incident Database in relation to the safety and security of artificial intelligence systems. It specifies the subject (AI Incident Database) and the context (safety and security of AI systems), making the intent clear and the question understandable. It does not rely on external references or additional context, thus meeting the criteria for independence. Therefore, it is specific, independent, and has a clear intent.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How does the AI Incident Database improve AI system safety?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on a broader inquiry regarding algorithmic and data-driven harms, while the second question specifically addresses input collection for the AI Bill of Rights, which is a narrower topic. This results in different depths and breadths of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the significance of the AI Incident Database in a broader context of research and development, while the second question specifically addresses its role in improving safety. This difference in focus leads to varying depths and breadths of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The White House Office of Science and Technology Policy (OSTP) collected input for the AI Bill of Rights through a yearlong process that involved seeking and distilling input from people across the country, including impacted communities, industry stakeholders, technology developers, experts across fields and sectors, and policymakers across the Federal government. This process included panel discussions, public listening sessions, private meetings, a formal request for information, and input to a publicly accessible email address.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Training data use diligence', 'Intellectual property risks', 'Data privacy risks', 'Content provenance', 'GAI structured public feedback']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What diligence should be conducted on training data use to assess intellectual property risks in AI systems?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the diligence required to assess intellectual property risks related to the use of training data in AI systems. It is specific and conveys a clear intent, focusing on the assessment of intellectual property risks. However, it could benefit from further clarification regarding what aspects of diligence are being referred to (e.g., legal review, data sourcing practices, compliance checks) to enhance answerability. Additionally, specifying the type of AI systems or the context in which this diligence is being applied could provide more clarity. Overall, the question is understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What diligence should be conducted on training data use to assess intellectual property risks in AI systems?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: What measures should be taken to evaluate the use of training data for assessing intellectual property risks in AI systems, considering both data accuracy and potential content manipulation?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and clear in its intent, asking for measures to evaluate training data in the context of intellectual property risks in AI systems. It specifies two important aspects: data accuracy and potential content manipulation, which helps in understanding the focus of the inquiry. The question is independent and does not rely on external references, making it understandable and answerable based on the details provided. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: How to assess training data for IP risks in AI, considering accuracy and content manipulation?\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions focus on assessing intellectual property risks related to training data in AI systems, but they differ in their approach and specific aspects of inquiry, such as the emphasis on diligence versus accuracy and content manipulation.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Conduct appropriate diligence on training data use to assess intellectual property risks in AI, including examining whether the use of proprietary or sensitive training data complies with applicable laws. Evaluate the accuracy, representativeness, relevance, and suitability of data used at different stages of the AI life cycle. Implement fact-checking techniques to verify the accuracy and veracity of information generated by AI systems, especially when the information comes from multiple or unknown sources. Develop and implement testing techniques to identify AI-produced content that might be indistinguishable from human-generated content. Implement plans for AI systems to undergo regular adversarial testing to identify vulnerabilities and potential manipulation or misuse.', 'verdict': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why is providing notice and explanation import...</td>\n",
       "      <td>[ \\n \\n  \\n \\n \\nNOTICE &amp; \\nEXPLANATION \\nWHY ...</td>\n",
       "      <td>Providing notice and explanations in the conte...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can designers, developers, and deployers o...</td>\n",
       "      <td>[ ­­­­­­­\\nALGORITHMIC DISCRIMINATION Protecti...</td>\n",
       "      <td>Designers, developers, and deployers of automa...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What information should be included in reporti...</td>\n",
       "      <td>[ \\n \\n \\n \\n \\n \\n \\nHUMAN ALTERNATIVES, \\nCO...</td>\n",
       "      <td>Reporting for automated systems used in sensit...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What mechanisms are in place for inventorying ...</td>\n",
       "      <td>[ \\n16 \\nGOVERN 1.5: Ongoing monitoring and pe...</td>\n",
       "      <td>Mechanisms for inventorying AI systems include...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What resources are provided in the National In...</td>\n",
       "      <td>[ \\n57 \\nNational Institute of Standards and T...</td>\n",
       "      <td>The National Institute of Standards and Techno...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Why is it important for both police and the pu...</td>\n",
       "      <td>[ \\n \\n  \\n \\n \\nNOTICE &amp; \\nEXPLANATION \\nWHY ...</td>\n",
       "      <td>Both police and the public deserve to understa...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can continuous monitoring be implemented t...</td>\n",
       "      <td>[ \\n29 \\nMS-1.1-006 \\nImplement continuous mon...</td>\n",
       "      <td>Continuous monitoring of GAI system impacts ca...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What sparked the state-wide biometrics morator...</td>\n",
       "      <td>[ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n...</td>\n",
       "      <td>A state-wide biometrics moratorium was sparked...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How can bias in artificial intelligence be ide...</td>\n",
       "      <td>[ \\n57 \\nNational Institute of Standards and T...</td>\n",
       "      <td>Towards a Standard for Identifying and Managin...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do advertisement delivery systems reinforc...</td>\n",
       "      <td>[ \\n \\n  \\nWHY THIS PRINCIPLE IS IMPORTANT\\nTh...</td>\n",
       "      <td>Advertisement delivery systems reinforce racia...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How does the AI Incident Database improve AI s...</td>\n",
       "      <td>[ \\n54 \\nAppendix B. References \\nAcemoglu, D....</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What's needed for a fair and accessible algori...</td>\n",
       "      <td>[ \\n \\n \\n \\n \\n \\n \\nWHAT SHOULD BE EXPECTED ...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do ballot curing laws protect voter rights?</td>\n",
       "      <td>[ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n...</td>\n",
       "      <td>Ballot curing laws in at least 24 states provi...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How to assess training data for IP risks in AI...</td>\n",
       "      <td>[ \\n27 \\nMP-4.1-010 \\nConduct appropriate dili...</td>\n",
       "      <td>Conduct appropriate diligence on training data...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What risks come from confabulated content in G...</td>\n",
       "      <td>[ \\n6 \\n2.2. Confabulation \\n“Confabulation” r...</td>\n",
       "      <td>Risks from confabulated content in GAI systems...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Why is regular adversarial testing important f...</td>\n",
       "      <td>[ \\n39 \\nMS-3.3-004 \\nProvide input for traini...</td>\n",
       "      <td>Regular adversarial testing is important for G...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How did the OSTP collect input for the AI Bill...</td>\n",
       "      <td>[ \\n \\n \\n \\n \\nSECTION TITLE\\nAPPENDIX\\nListe...</td>\n",
       "      <td>The White House Office of Science and Technolo...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What mechanisms are needed for ethical use of ...</td>\n",
       "      <td>[ \\n \\n \\n \\n \\n \\n \\nHUMAN ALTERNATIVES, \\nCO...</td>\n",
       "      <td>The expectations for automated systems in sens...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What's the specific requirement for private en...</td>\n",
       "      <td>[ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n...</td>\n",
       "      <td>Private entities in Illinois are required to p...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How are environmental impacts and sustainabili...</td>\n",
       "      <td>[ \\n37 \\nMS-2.11-005 \\nAssess the proportion o...</td>\n",
       "      <td>Environmental impacts and sustainability of AI...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   Why is providing notice and explanation import...   \n",
       "1   How can designers, developers, and deployers o...   \n",
       "2   What information should be included in reporti...   \n",
       "3   What mechanisms are in place for inventorying ...   \n",
       "4   What resources are provided in the National In...   \n",
       "5   Why is it important for both police and the pu...   \n",
       "6   How can continuous monitoring be implemented t...   \n",
       "7   What sparked the state-wide biometrics morator...   \n",
       "8   How can bias in artificial intelligence be ide...   \n",
       "9   How do advertisement delivery systems reinforc...   \n",
       "10  How does the AI Incident Database improve AI s...   \n",
       "11  What's needed for a fair and accessible algori...   \n",
       "12    How do ballot curing laws protect voter rights?   \n",
       "13  How to assess training data for IP risks in AI...   \n",
       "14  What risks come from confabulated content in G...   \n",
       "15  Why is regular adversarial testing important f...   \n",
       "16  How did the OSTP collect input for the AI Bill...   \n",
       "17  What mechanisms are needed for ethical use of ...   \n",
       "18  What's the specific requirement for private en...   \n",
       "19  How are environmental impacts and sustainabili...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [ \\n \\n  \\n \\n \\nNOTICE & \\nEXPLANATION \\nWHY ...   \n",
       "1   [ ­­­­­­­\\nALGORITHMIC DISCRIMINATION Protecti...   \n",
       "2   [ \\n \\n \\n \\n \\n \\n \\nHUMAN ALTERNATIVES, \\nCO...   \n",
       "3   [ \\n16 \\nGOVERN 1.5: Ongoing monitoring and pe...   \n",
       "4   [ \\n57 \\nNational Institute of Standards and T...   \n",
       "5   [ \\n \\n  \\n \\n \\nNOTICE & \\nEXPLANATION \\nWHY ...   \n",
       "6   [ \\n29 \\nMS-1.1-006 \\nImplement continuous mon...   \n",
       "7   [ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n...   \n",
       "8   [ \\n57 \\nNational Institute of Standards and T...   \n",
       "9   [ \\n \\n  \\nWHY THIS PRINCIPLE IS IMPORTANT\\nTh...   \n",
       "10  [ \\n54 \\nAppendix B. References \\nAcemoglu, D....   \n",
       "11  [ \\n \\n \\n \\n \\n \\n \\nWHAT SHOULD BE EXPECTED ...   \n",
       "12  [ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n...   \n",
       "13  [ \\n27 \\nMP-4.1-010 \\nConduct appropriate dili...   \n",
       "14  [ \\n6 \\n2.2. Confabulation \\n“Confabulation” r...   \n",
       "15  [ \\n39 \\nMS-3.3-004 \\nProvide input for traini...   \n",
       "16  [ \\n \\n \\n \\n \\nSECTION TITLE\\nAPPENDIX\\nListe...   \n",
       "17  [ \\n \\n \\n \\n \\n \\n \\nHUMAN ALTERNATIVES, \\nCO...   \n",
       "18  [ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n...   \n",
       "19  [ \\n37 \\nMS-2.11-005 \\nAssess the proportion o...   \n",
       "\n",
       "                                         ground_truth evolution_type  \\\n",
       "0   Providing notice and explanations in the conte...         simple   \n",
       "1   Designers, developers, and deployers of automa...         simple   \n",
       "2   Reporting for automated systems used in sensit...         simple   \n",
       "3   Mechanisms for inventorying AI systems include...         simple   \n",
       "4   The National Institute of Standards and Techno...         simple   \n",
       "5   Both police and the public deserve to understa...         simple   \n",
       "6   Continuous monitoring of GAI system impacts ca...         simple   \n",
       "7   A state-wide biometrics moratorium was sparked...         simple   \n",
       "8   Towards a Standard for Identifying and Managin...         simple   \n",
       "9   Advertisement delivery systems reinforce racia...         simple   \n",
       "10  The answer to given question is not present in...  multi_context   \n",
       "11  The answer to given question is not present in...  multi_context   \n",
       "12  Ballot curing laws in at least 24 states provi...  multi_context   \n",
       "13  Conduct appropriate diligence on training data...  multi_context   \n",
       "14  Risks from confabulated content in GAI systems...  multi_context   \n",
       "15  Regular adversarial testing is important for G...  multi_context   \n",
       "16  The White House Office of Science and Technolo...  multi_context   \n",
       "17  The expectations for automated systems in sens...  multi_context   \n",
       "18  Private entities in Illinois are required to p...      reasoning   \n",
       "19  Environmental impacts and sustainability of AI...      reasoning   \n",
       "\n",
       "                                             metadata  episode_done  \n",
       "0   [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "1   [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "2   [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "3   [{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...          True  \n",
       "4   [{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...          True  \n",
       "5   [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "6   [{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...          True  \n",
       "7   [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "8   [{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...          True  \n",
       "9   [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "10  [{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...          True  \n",
       "11  [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "12  [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "13  [{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...          True  \n",
       "14  [{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...          True  \n",
       "15  [{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...          True  \n",
       "16  [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "17  [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "18  [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "19  [{'source': 'data/NIST.AI.600-1.pdf', 'file_pa...          True  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "\n",
    "filepath_NIST = \"data/NIST.AI.600-1.pdf\"\n",
    "filepath_Blueprint = \"data/Blueprint-for-an-AI-Bill-of-Rights.pdf\"\n",
    "\n",
    "documents_NIST = PyMuPDFLoader(filepath_NIST).load()\n",
    "documents_Blueprint = PyMuPDFLoader(filepath_Blueprint).load()\n",
    "documents = documents_NIST + documents_Blueprint\n",
    "\n",
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\", tags=[\"base_llm\"]) \n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "distributions = {\n",
    "    simple: 0.5,\n",
    "    multi_context: 0.4,\n",
    "    reasoning: 0.1\n",
    "}\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(documents, 20, distributions, with_debugging_logs=True)\n",
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langsmith import Client\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = \"Implications of AI\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Questions about the implications of AI\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in testset.to_pandas().iterrows():\n",
    "  client.create_example(\n",
    "      inputs={\n",
    "          \"question\": test[1][\"question\"]\n",
    "      },\n",
    "      outputs={\n",
    "          \"answer\": test[1][\"ground_truth\"]\n",
    "      },\n",
    "      metadata={\n",
    "          \"context\": test[0]\n",
    "      },\n",
    "      dataset_id=dataset.id\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is providing notice and explanation important in the context of automated decision-making processes?\n",
      "Providing notice and explanations in the context of automated decision-making processes is important because it allows individuals to understand how automated systems are impacting their lives. Without clear explanations, people may not know why certain decisions are made, leading to a lack of transparency and accountability. Notice and explanations also help experts verify the reasonableness of recommendations before they are enacted, ensuring safety and efficacy. In order to guard against potential harms, it is crucial for the public to know if automated systems are being used and how they are making decisions that affect rights, opportunities, and access. Clear and valid explanations should be recognized as a baseline requirement to build trust and confidence in the use of automated systems.\n"
     ]
    }
   ],
   "source": [
    "test_questions = testset.to_pandas()[\"question\"].values.tolist()\n",
    "test_groundtruths = testset.to_pandas()[\"ground_truth\"].values.tolist()\n",
    "\n",
    "print(test_questions[0])\n",
    "print(test_groundtruths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': AIMessage(content=\"I don't know.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 1515, 'total_tokens': 1519, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-f51c923b-1e1d-4a11-a971-2b6d7fbb9aee-0', usage_metadata={'input_tokens': 1515, 'output_tokens': 4, 'total_tokens': 1519}), 'context': [Document(metadata={'source': 'data/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'file_path': 'data/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 3, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': '', 'id': '8b974c51-84d5-4da7-a434-077c375bad2b', '_id': 'f31eec5e22a84b5e9c569274299bc556', '_collection_name': 'Implications of AI'}, page_content='international community spoke up about both the promises and potential harms of these technologies, and \\nplayed a central role in shaping the Blueprint for an AI Bill of Rights. The core messages gleaned from these \\ndiscussions include that AI has transformative potential to improve Americans’ lives, and that preventing the \\nharms of these technologies is both necessary and achievable. The Appendix includes a full list of public engage-\\nments. \\n4'), Document(metadata={'source': 'data/NIST.AI.600-1.pdf', 'file_path': 'data/NIST.AI.600-1.pdf', 'page': 59, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'id': '1e84b086-6a92-495f-a99e-6037ba3848b2', '_id': 'ed29be82a6d04a3ab19ad76a0ab1c008', '_collection_name': 'Implications of AI'}, page_content='Luccioni, A. et al. (2023) Power Hungry Processing: Watts Driving the Cost of AI Deployment? arXiv. \\nhttps://arxiv.org/pdf/2311.16863 \\nMouton, C. et al. (2024) The Operational Risks of AI in Large-Scale Biological Attacks. RAND. \\nhttps://www.rand.org/pubs/research_reports/RRA2977-2.html. \\nNicoletti, L. et al. (2023) Humans Are Biased. Generative Ai Is Even Worse. Bloomberg. \\nhttps://www.bloomberg.com/graphics/2023-generative-ai-bias/. \\nNational Institute of Standards and Technology (2024) Adversarial Machine Learning: A Taxonomy and \\nTerminology of Attacks and Mitigations https://csrc.nist.gov/pubs/ai/100/2/e2023/ﬁnal \\nNational Institute of Standards and Technology (2023) AI Risk Management Framework.'), Document(metadata={'source': 'data/NIST.AI.600-1.pdf', 'file_path': 'data/NIST.AI.600-1.pdf', 'page': 6, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'id': '994ede06-b06c-4daa-8df3-1b9ea0e0dc87', '_id': 'caf0616587304bfead2006a284d80418', '_collection_name': 'Implications of AI'}, page_content='3 \\nthe abuse, misuse, and unsafe repurposing by humans (adversarial or not), and others result \\nfrom interactions between a human and an AI system.  \\n• \\nTime scale: GAI risks may materialize abruptly or across extended periods. Examples include \\nimmediate (and/or prolonged) emotional harm and potential risks to physical safety due to the \\ndistribution of harmful deepfake images, or the long-term eﬀect of disinformation on societal \\ntrust in public institutions. \\nThe presence of risks and where they fall along the dimensions above will vary depending on the \\ncharacteristics of the GAI model, system, or use case at hand. These characteristics include but are not'), Document(metadata={'source': 'data/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'file_path': 'data/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 28, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': '', 'id': 'ce9b9fb9-0254-4461-b615-4ae2054f6859', '_id': '94ef1f926f35448da170cbe58d5c180e', '_collection_name': 'Implications of AI'}, page_content='in Artificial Intelligence.59 The special publication: describes the stakes and challenges of bias in artificial \\nintelligence and provides examples of how and why it can chip away at public trust; identifies three categories \\nof bias in AI – systemic, statistical, and human – and describes how and where they contribute to harms; and \\ndescribes three broad challenges for mitigating bias – datasets, testing and evaluation, and human factors – and \\nintroduces preliminary guidance for addressing them. Throughout, the special publication takes a socio-\\ntechnical perspective to identifying and managing AI bias. \\n29\\nAlgorithmic \\nDiscrimination \\nProtections')]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "primary_qa_llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"Implications of AI\"\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Create a vector store\n",
    "vector_db = VectorDatabase()\n",
    "vector_db = await vector_db.abuild_from_list(split_documents_NIST)\n",
    "vector_db = await vector_db.abuild_from_list(split_documents_Blueprint)\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = \"\"\" \\\n",
    "Use the provided context to answer the user's query.\n",
    "You may not answer the user's query unless there is specific context in the following text.\n",
    "If you do not know the answer, or cannot answer, please respond with \"I don't know\".\n",
    "Context:\n",
    "{context}\n",
    "User Query:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
    "\n",
    "retrieval_augmented_qa_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "\n",
    "result = retrieval_augmented_qa_chain.invoke({\"question\" : \"Is AI a threat to humanity?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'NIST stands for the National Institute of Standards and Technology. It develops measurements, technology, tools, and standards to advance reliable, safe, transparent, explainable, privacy-enhanced, and fair artificial intelligence (AI). NIST has been conducting work on AI for more than a decade and is involved in efforts to fulfill the 2023 Executive Order on Safe, Secure, and Trustworthy AI.', 'context': [Document(metadata={'source': 'data/NIST.AI.600-1.pdf', 'file_path': 'data/NIST.AI.600-1.pdf', 'page': 2, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'id': '516fbe2f-9208-4540-a9ee-6d03200f3341', '_id': '34da52cab37f42e6a9d4a7289a074e5b', '_collection_name': 'Implications of AI'}, page_content='About AI at NIST: The National Institute of Standards and Technology (NIST) develops measurements, \\ntechnology, tools, and standards to advance reliable, safe, transparent, explainable, privacy-enhanced, \\nand fair artiﬁcial intelligence (AI) so that its full commercial and societal beneﬁts can be realized without \\nharm to people or the planet. NIST, which has conducted both fundamental and applied work on AI for \\nmore than a decade, is also helping to fulﬁll the 2023 Executive Order on Safe, Secure, and Trustworthy \\nAI. NIST established the U.S. AI Safety Institute and the companion AI Safety Institute Consortium to \\ncontinue the eﬀorts set in motion by the E.O. to build the science necessary for safe, secure, and'), Document(metadata={'source': 'data/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'file_path': 'data/Blueprint-for-an-AI-Bill-of-Rights.pdf', 'page': 20, 'total_pages': 73, 'format': 'PDF 1.6', 'title': 'Blueprint for an AI Bill of Rights', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe Illustrator 26.3 (Macintosh)', 'producer': 'iLovePDF', 'creationDate': \"D:20220920133035-04'00'\", 'modDate': \"D:20221003104118-04'00'\", 'trapped': '', 'id': '5eeb948c-2bae-4ae4-845a-3e3f2b40399a', '_id': '23e3feeb96494e55afd7b02244731e75', '_collection_name': 'Implications of AI'}, page_content='evaluation of AI products, services, and systems. The NIST framework is being developed through a consensus-\\ndriven, open, transparent, and collaborative process that includes workshops and other opportunities to provide \\ninput. The NIST framework aims to foster the development of innovative approaches to address \\ncharacteristics of trustworthiness including accuracy, explainability and interpretability, reliability, privacy, \\nrobustness, safety, security (resilience), and mitigation of unintended and/or harmful bias, as well as of \\nharmful \\nuses. \\nThe \\nNIST \\nframework \\nwill \\nconsider \\nand \\nencompass \\nprinciples \\nsuch \\nas \\ntransparency, accountability, and fairness during pre-design, design and development, deployment, use,'), Document(metadata={'source': 'data/NIST.AI.600-1.pdf', 'file_path': 'data/NIST.AI.600-1.pdf', 'page': 1, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'id': 'a926e00a-d9dd-4d66-aad7-4431a2d8bf2c', '_id': 'c7c4702067404ddc84c8433b738a2a20', '_collection_name': 'Implications of AI'}, page_content='NIST Trustworthy and Responsible AI  \\nNIST AI 600-1 \\nArtificial Intelligence Risk Management \\nFramework: Generative Artificial \\nIntelligence Profile \\n \\n \\n \\nThis publication is available free of charge from: \\nhttps://doi.org/10.6028/NIST.AI.600-1 \\n \\nJuly 2024 \\n \\n \\n \\n \\nU.S. Department of Commerce  \\nGina M. Raimondo, Secretary \\nNational Institute of Standards and Technology  \\nLaurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology'), Document(metadata={'source': 'data/NIST.AI.600-1.pdf', 'file_path': 'data/NIST.AI.600-1.pdf', 'page': 2, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'id': '8f47a9e4-7e10-4077-affd-4f42b39a3404', '_id': 'ef04ecc1df404735a34b050da3f8e97e', '_collection_name': 'Implications of AI'}, page_content='100 Bureau Drive (Mail Stop 8900) Gaithersburg, MD 20899-8900 \\nAdditional Information \\nAdditional information about this publication and other NIST AI publications are available at \\nhttps://airc.nist.gov/Home. \\n \\nDisclaimer: Certain commercial entities, equipment, or materials may be identiﬁed in this document in \\norder to adequately describe an experimental procedure or concept. Such identiﬁcation is not intended to \\nimply recommendation or endorsement by the National Institute of Standards and Technology, nor is it \\nintended to imply that the entities, materials, or equipment are necessarily the best available for the \\npurpose. Any mention of commercial, non-proﬁt, academic partners, or their products, or references is')]}\n"
     ]
    }
   ],
   "source": [
    "result = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is NIST?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A concern of AI is that it can produce outputs that may be erroneous, leading to ill-founded decision-making or amplifying harmful biases. Additionally, AI incidents can cause harm to health, disrupt critical infrastructure, violate human rights, or lead to misinformation.', 'context': [Document(metadata={'source': 'data/NIST.AI.600-1.pdf', 'file_path': 'data/NIST.AI.600-1.pdf', 'page': 7, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'id': '0e3c2613-ec8e-40c1-96b5-3453c2d22503', '_id': '04147c3070094fc3affb7f1e7b26043c', '_collection_name': 'Implications of AI'}, page_content='outputs, which may be erroneous, lead to ill-founded decision-making, or amplify harmful \\nbiases.  \\n7. Human-AI Conﬁguration: Arrangements of or interactions between a human and an AI system \\nwhich can result in the human inappropriately anthropomorphizing GAI systems or experiencing \\nalgorithmic aversion, automation bias, over-reliance, or emotional entanglement with GAI \\nsystems. \\n8. Information Integrity: Lowered barrier to entry to generate and support the exchange and \\nconsumption of content which may not distinguish fact from opinion or ﬁction or acknowledge \\nuncertainties, or could be leveraged for large-scale dis- and mis-information campaigns.'), Document(metadata={'source': 'data/NIST.AI.600-1.pdf', 'file_path': 'data/NIST.AI.600-1.pdf', 'page': 55, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'id': 'c1cac7e7-3571-414c-8507-677a4b989e75', '_id': 'c78d3f70fe3b4e60b04df61eba7c2bdc', '_collection_name': 'Implications of AI'}, page_content='Overview \\nAI incidents can be deﬁned as an “event, circumstance, or series of events where the development, use, \\nor malfunction of one or more AI systems directly or indirectly contributes to one of the following harms: \\ninjury or harm to the health of a person or groups of people (including psychological harms and harms to \\nmental health); disruption of the management and operation of critical infrastructure; violations of \\nhuman rights or a breach of obligations under applicable law intended to protect fundamental, labor, \\nand intellectual property rights; or harm to property, communities, or the environment.” AI incidents can \\noccur in the aggregate (i.e., for systemic discrimination) or acutely (i.e., for one individual).'), Document(metadata={'source': 'data/NIST.AI.600-1.pdf', 'file_path': 'data/NIST.AI.600-1.pdf', 'page': 31, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'id': 'add7d760-0642-4ba4-b3d4-29a32c1dc2b7', '_id': 'b1132d2f073e4eaf959b3b123f118cab', '_collection_name': 'Implications of AI'}, page_content='impacts. \\nHuman-AI Conﬁguration; Value \\nChain and Component Integration \\nAI Actor Tasks: AI Deployment, AI Design, AI Impact Assessment, Aﬀected Individuals and Communities, Domain Experts, End-\\nUsers, Human Factors, Operation and Monitoring  \\n \\nMEASURE 1.1: Approaches and metrics for measurement of AI risks enumerated during the MAP function are selected for \\nimplementation starting with the most signiﬁcant AI risks. The risks or trustworthiness characteristics that will not – or cannot – be \\nmeasured are properly documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-1.1-001 Employ methods to trace the origin and modiﬁcations of digital content. \\nInformation Integrity \\nMS-1.1-002'), Document(metadata={'source': 'data/NIST.AI.600-1.pdf', 'file_path': 'data/NIST.AI.600-1.pdf', 'page': 6, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'id': '994ede06-b06c-4daa-8df3-1b9ea0e0dc87', '_id': '8fe8c579a5424202aeaed5014f43d3ba', '_collection_name': 'Implications of AI'}, page_content='3 \\nthe abuse, misuse, and unsafe repurposing by humans (adversarial or not), and others result \\nfrom interactions between a human and an AI system.  \\n• \\nTime scale: GAI risks may materialize abruptly or across extended periods. Examples include \\nimmediate (and/or prolonged) emotional harm and potential risks to physical safety due to the \\ndistribution of harmful deepfake images, or the long-term eﬀect of disinformation on societal \\ntrust in public institutions. \\nThe presence of risks and where they fall along the dimensions above will vary depending on the \\ncharacteristics of the GAI model, system, or use case at hand. These characteristics include but are not')]}\n"
     ]
    }
   ],
   "source": [
    "result = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is a concern of AI?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for question in test_questions:\n",
    "  response = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
    "  answers.append(response[\"response\"].content)\n",
    "  contexts.append([context.page_content for context in response[\"context\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Why is providing notice and explanation important in the context of automated decision-making processes?',\n",
       " 'answer': 'Providing notice and explanation is important in the context of automated decision-making processes because it helps individuals understand if and how automated systems are being used to influence significant outcomes in their lives, such as employment, credit, and legal decisions. Clear and accessible notice allows the public to be informed about the use of these systems, which is essential for ensuring accountability and trust. \\n\\nMoreover, explanations of how and why decisions are made by automated systems are crucial for validating the fairness and accuracy of those decisions. This transparency enables individuals to contest or challenge decisions that may adversely affect them, as it provides the necessary information to understand the reasoning behind the outcomes. Without such notice and explanations, people may feel powerless and unable to address the impacts of automated systems on their rights and opportunities.',\n",
       " 'contexts': [\" \\n \\n  \\n \\n \\nNOTICE & \\nEXPLANATION \\nWHY THIS PRINCIPLE IS IMPORTANT\\nThis section provides a brief summary of the problems which the principle seeks to address and protect \\nagainst, including illustrative examples. \\nAutomated systems now determine opportunities, from employment to credit, and directly shape the American \\npublic’s experiences, from the courtroom to online classrooms, in ways that profoundly impact people’s lives. But this \\nexpansive impact is not always visible. An applicant might not know whether a person rejected their resume or a \\nhiring algorithm moved them to the bottom of the list. A defendant in the courtroom might not know if a judge deny\\xad\\ning their bail is informed by an automated system that labeled them “high risk.” From correcting errors to contesting \\ndecisions, people are often denied the knowledge they need to address the impact of automated systems on their lives. \\nNotice and explanations also serve an important safety and efficacy purpose, allowing experts to verify the reasonable\\xad\\nness of a recommendation before enacting it. \\nIn order to guard against potential harms, the American public needs to know if an automated system is being used. \\nClear, brief, and understandable notice is a prerequisite for achieving the other protections in this framework. Like\\xad\\nwise, the public is often unable to ascertain how or why an automated system has made a decision or contributed to a \\nparticular outcome. The decision-making processes of automated systems tend to be opaque, complex, and, therefore, \\nunaccountable, whether by design or by omission. These factors can make explanations both more challenging and \\nmore important, and should not be used as a pretext to avoid explaining important decisions to the people impacted \\nby those choices. In the context of automated systems, clear and valid explanations should be recognized as a baseline \\nrequirement. \\nProviding notice has long been a standard practice, and in many cases is a legal requirement, when, for example, \\nmaking a video recording of someone (outside of a law enforcement or national security context). In some cases, such \\nas credit, lenders are required to provide notice and explanation to consumers. Techniques used to automate the \\nprocess of explaining such systems are under active research and improvement and such explanations can take many \\nforms. Innovative companies and researchers are rising to the challenge and creating and deploying explanatory \\nsystems that can help the public better understand decisions that impact them. \\nWhile notice and explanation requirements are already in place in some sectors or situations, the American public \\ndeserve to know consistently and across sectors if an automated system is being used in a way that impacts their rights, \\nopportunities, or access. This knowledge should provide confidence in how the public is being treated, and trust in the \\nvalidity and reasonable use of automated systems. \\n•\\nA lawyer representing an older client with disabilities who had been cut off from Medicaid-funded home\\nhealth-care assistance couldn't determine why, especially since the decision went against historical access\\npractices. In a court hearing, the lawyer learned from a witness that the state in which the older client\\nlived had recently adopted a new algorithm to determine eligibility.83 The lack of a timely explanation made it\\nharder to understand and contest the decision.\\n•\\nA formal child welfare investigation is opened against a parent based on an algorithm and without the parent\\never being notified that data was being collected and used as part of an algorithmic child maltreatment\\nrisk assessment.84 The lack of notice or an explanation makes it harder for those performing child\\nmaltreatment assessments to validate the risk assessment and denies parents knowledge that could help them\\ncontest a decision.\\n41\\n\",\n",
       "  \" \\n \\n \\n \\n \\n \\nNOTICE & \\nEXPLANATION \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nAn automated system should provide demonstrably clear, timely, understandable, and accessible notice of use, and \\nexplanations as to how and why a decision was made or an action was taken by the system. These expectations are \\nexplained below. \\nProvide clear, timely, understandable, and accessible notice of use and explanations \\xad\\nGenerally accessible plain language documentation. The entity responsible for using the automated \\nsystem should ensure that documentation describing the overall system (including any human components) is \\npublic and easy to find. The documentation should describe, in plain language, how the system works and how \\nany automated component is used to determine an action or decision. It should also include expectations about \\nreporting described throughout this framework, such as the algorithmic impact assessments described as \\npart of Algorithmic Discrimination Protections. \\nAccountable. Notices should clearly identify the entity responsible for designing each component of the \\nsystem and the entity using it. \\nTimely and up-to-date. Users should receive notice of the use of automated systems in advance of using or \\nwhile being impacted by the technology. An explanation should be available with the decision itself, or soon \\nthereafter. Notice should be kept up-to-date and people impacted by the system should be notified of use case \\nor key functionality changes. \\nBrief and clear. Notices and explanations should be assessed, such as by research on users’ experiences, \\nincluding user testing, to ensure that the people using or impacted by the automated system are able to easily \\nfind notices and explanations, read them quickly, and understand and act on them. This includes ensuring that \\nnotices and explanations are accessible to users with disabilities and are available in the language(s) and read-\\ning level appropriate for the audience. Notices and explanations may need to be available in multiple forms, \\n(e.g., on paper, on a physical sign, or online), in order to meet these expectations and to be accessible to the \\nAmerican public. \\nProvide explanations as to how and why a decision was made or an action was taken by an \\nautomated system \\nTailored to the purpose. Explanations should be tailored to the specific purpose for which the user is \\nexpected to use the explanation, and should clearly state that purpose. An informational explanation might \\ndiffer from an explanation provided to allow for the possibility of recourse, an appeal, or one provided in the \\ncontext of a dispute or contestation process. For the purposes of this framework, 'explanation' should be \\nconstrued broadly. An explanation need not be a plain-language statement about causality but could consist of \\nany mechanism that allows the recipient to build the necessary understanding and intuitions to achieve the \\nstated purpose. Tailoring should be assessed (e.g., via user experience research). \\nTailored to the target of the explanation. Explanations should be targeted to specific audiences and \\nclearly state that audience. An explanation provided to the subject of a decision might differ from one provided \\nto an advocate, or to a domain expert or decision maker. Tailoring should be assessed (e.g., via user experience \\nresearch). \\n43\\n\",\n",
       "  ' \\nYou should know that an automated system is being used, \\nand understand how and why it contributes to outcomes \\nthat impact you. Designers, developers, and deployers of automat\\xad\\ned systems should provide generally accessible plain language docu\\xad\\nmentation including clear descriptions of the overall system func\\xad\\ntioning and the role automation plays, notice that such systems are in \\nuse, the individual or organization responsible for the system, and ex\\xad\\nplanations of outcomes that are clear, timely, and accessible. Such \\nnotice should be kept up-to-date and people impacted by the system \\nshould be notified of significant use case or key functionality chang\\xad\\nes. You should know how and why an outcome impacting you was de\\xad\\ntermined by an automated system, including when the automated \\nsystem is not the sole input determining the outcome. Automated \\nsystems should provide explanations that are technically valid, \\nmeaningful and useful to you and to any operators or others who \\nneed to understand the system, and calibrated to the level of risk \\nbased on the context. Reporting that includes summary information \\nabout these automated systems in plain language and assessments of \\nthe clarity and quality of the notice and explanations should be made \\npublic whenever possible.   \\nNOTICE AND EXPLANATION\\n40\\n',\n",
       "  \" \\n \\n \\n \\n \\n \\nNOTICE & \\nEXPLANATION \\nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \\ntechnical standards and practices that are tailored for particular sectors and contexts. \\nTailored to the level of risk. An assessment should be done to determine the level of risk of the auto\\xad\\nmated system. In settings where the consequences are high as determined by a risk assessment, or extensive \\noversight is expected (e.g., in criminal justice or some public sector settings), explanatory mechanisms should \\nbe built into the system design so that the system’s full behavior can be explained in advance (i.e., only fully \\ntransparent models should be used), rather than as an after-the-decision interpretation. In other settings, the \\nextent of explanation provided should be tailored to the risk level. \\nValid. The explanation provided by a system should accurately reflect the factors and the influences that led \\nto a particular decision, and should be meaningful for the particular customization based on purpose, target, \\nand level of risk. While approximation and simplification may be necessary for the system to succeed based on \\nthe explanatory purpose and target of the explanation, or to account for the risk of fraud or other concerns \\nrelated to revealing decision-making information, such simplifications should be done in a scientifically \\nsupportable way. Where appropriate based on the explanatory system, error ranges for the explanation should \\nbe calculated and included in the explanation, with the choice of presentation of such information balanced \\nwith usability and overall interface complexity concerns. \\nDemonstrate protections for notice and explanation \\nReporting. Summary reporting should document the determinations made based on the above consider\\xad\\nations, including: the responsible entities for accountability purposes; the goal and use cases for the system, \\nidentified users, and impacted populations; the assessment of notice clarity and timeliness; the assessment of \\nthe explanation's validity and accessibility; the assessment of the level of risk; and the account and assessment \\nof how explanations are tailored, including to the purpose, the recipient of the explanation, and the level of \\nrisk. Individualized profile information should be made readily available to the greatest extent possible that \\nincludes explanations for any system impacts or inferences. Reporting should be provided in a clear plain \\nlanguage and machine-readable manner. \\n44\\n\"],\n",
       " 'ground_truth': 'Providing notice and explanations in the context of automated decision-making processes is important because it allows individuals to understand how automated systems are impacting their lives. Without clear explanations, people may not know why certain decisions are made, leading to a lack of transparency and accountability. Notice and explanations also help experts verify the reasonableness of recommendations before they are enacted, ensuring safety and efficacy. In order to guard against potential harms, it is crucial for the public to know if automated systems are being used and how they are making decisions that affect rights, opportunities, and access. Clear and valid explanations should be recognized as a baseline requirement to build trust and confidence in the use of automated systems.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "response_dataset = Dataset.from_dict({\n",
    "    \"question\" : test_questions,\n",
    "    \"answer\" : answers,\n",
    "    \"contexts\" : contexts,\n",
    "    \"ground_truth\" : test_groundtruths\n",
    "})\n",
    "response_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de376fe2ad9640bd857b1e1a884b9d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = evaluate(response_dataset, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>answer_correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why is providing notice and explanation import...</td>\n",
       "      <td>[ \\n \\n  \\n \\n \\nNOTICE &amp; \\nEXPLANATION \\nWHY ...</td>\n",
       "      <td>Providing notice and explanation is important ...</td>\n",
       "      <td>Providing notice and explanations in the conte...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996779</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.747138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can designers, developers, and deployers o...</td>\n",
       "      <td>[ ­­­­­­­\\nALGORITHMIC DISCRIMINATION Protecti...</td>\n",
       "      <td>Designers, developers, and deployers of automa...</td>\n",
       "      <td>Designers, developers, and deployers of automa...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976883</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.653547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What information should be included in reporti...</td>\n",
       "      <td>[ \\n \\n \\n \\n \\n \\n \\nDATA PRIVACY \\nWHAT SHOU...</td>\n",
       "      <td>Reporting for automated systems used in sensit...</td>\n",
       "      <td>Reporting for automated systems used in sensit...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.266613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What mechanisms are in place for inventorying ...</td>\n",
       "      <td>[ \\n16 \\nGOVERN 1.5: Ongoing monitoring and pe...</td>\n",
       "      <td>Mechanisms for inventorying AI systems include...</td>\n",
       "      <td>Mechanisms for inventorying AI systems include...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What resources are provided in the National In...</td>\n",
       "      <td>[ \\nNIST Trustworthy and Responsible AI  \\nNIS...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>The National Institute of Standards and Techno...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.179853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Why is it important for both police and the pu...</td>\n",
       "      <td>[ \\n \\n  \\n \\n \\nNOTICE &amp; \\nEXPLANATION \\nWHY ...</td>\n",
       "      <td>It is important for both police and the public...</td>\n",
       "      <td>Both police and the public deserve to understa...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995294</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.784810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can continuous monitoring be implemented t...</td>\n",
       "      <td>[ \\n36 \\nMEASURE 2.11: Fairness and bias – as ...</td>\n",
       "      <td>Continuous monitoring of GAI system impacts ca...</td>\n",
       "      <td>Continuous monitoring of GAI system impacts ca...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.898330</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.761270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What sparked the state-wide biometrics morator...</td>\n",
       "      <td>[  \\n  \\nAPPENDIX\\nPanel 3: Equal Opportunitie...</td>\n",
       "      <td>The state-wide biometrics moratorium was spark...</td>\n",
       "      <td>A state-wide biometrics moratorium was sparked...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.529175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How can bias in artificial intelligence be ide...</td>\n",
       "      <td>[ \\n \\n \\nAbout AI at NIST: The National Insti...</td>\n",
       "      <td>According to the National Institute of Standar...</td>\n",
       "      <td>Towards a Standard for Identifying and Managin...</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.951132</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.227159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do advertisement delivery systems reinforc...</td>\n",
       "      <td>[ \\n \\n  \\nWHY THIS PRINCIPLE IS IMPORTANT\\nTh...</td>\n",
       "      <td>Advertisement delivery systems reinforce racia...</td>\n",
       "      <td>Advertisement delivery systems reinforce racia...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.997382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How does the AI Incident Database improve AI s...</td>\n",
       "      <td>[ \\n53 \\nDocumenting, reporting, and sharing i...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What's needed for a fair and accessible algori...</td>\n",
       "      <td>[ \\n \\n \\n \\n \\n \\n \\nWHAT SHOULD BE EXPECTED ...</td>\n",
       "      <td>A fair and accessible algorithmic impact asses...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971865</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do ballot curing laws protect voter rights?</td>\n",
       "      <td>[ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n...</td>\n",
       "      <td>Ballot curing laws protect voter rights by req...</td>\n",
       "      <td>Ballot curing laws in at least 24 states provi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978773</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How to assess training data for IP risks in AI...</td>\n",
       "      <td>[ \\n11 \\nvalue chain (e.g., data inputs, proce...</td>\n",
       "      <td>The context provided mentions the importance o...</td>\n",
       "      <td>Conduct appropriate diligence on training data...</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.928923</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.683369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What risks come from confabulated content in G...</td>\n",
       "      <td>[ \\n6 \\n2.2. Confabulation \\n“Confabulation” r...</td>\n",
       "      <td>Confabulated content in GAI systems poses sign...</td>\n",
       "      <td>Risks from confabulated content in GAI systems...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972048</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Why is regular adversarial testing important f...</td>\n",
       "      <td>[ \\n48 \\n• Data protection \\n• Data retention ...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>Regular adversarial testing is important for G...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.179221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How did the OSTP collect input for the AI Bill...</td>\n",
       "      <td>[ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAbo...</td>\n",
       "      <td>The OSTP collected input for the AI Bill of Ri...</td>\n",
       "      <td>The White House Office of Science and Technolo...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What mechanisms are needed for ethical use of ...</td>\n",
       "      <td>[ \\n \\n \\n \\n \\n \\n \\nHUMAN ALTERNATIVES, \\nCO...</td>\n",
       "      <td>The mechanisms needed for ethical use of autom...</td>\n",
       "      <td>The expectations for automated systems in sens...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971665</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.737533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What's the specific requirement for private en...</td>\n",
       "      <td>[ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n...</td>\n",
       "      <td>Private entities in Illinois are required to p...</td>\n",
       "      <td>Private entities in Illinois are required to p...</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.956440</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How are environmental impacts and sustainabili...</td>\n",
       "      <td>[ \\n37 \\nMS-2.11-005 \\nAssess the proportion o...</td>\n",
       "      <td>Environmental impact and sustainability of AI ...</td>\n",
       "      <td>Environmental impacts and sustainability of AI...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.954457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.893801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   Why is providing notice and explanation import...   \n",
       "1   How can designers, developers, and deployers o...   \n",
       "2   What information should be included in reporti...   \n",
       "3   What mechanisms are in place for inventorying ...   \n",
       "4   What resources are provided in the National In...   \n",
       "5   Why is it important for both police and the pu...   \n",
       "6   How can continuous monitoring be implemented t...   \n",
       "7   What sparked the state-wide biometrics morator...   \n",
       "8   How can bias in artificial intelligence be ide...   \n",
       "9   How do advertisement delivery systems reinforc...   \n",
       "10  How does the AI Incident Database improve AI s...   \n",
       "11  What's needed for a fair and accessible algori...   \n",
       "12    How do ballot curing laws protect voter rights?   \n",
       "13  How to assess training data for IP risks in AI...   \n",
       "14  What risks come from confabulated content in G...   \n",
       "15  Why is regular adversarial testing important f...   \n",
       "16  How did the OSTP collect input for the AI Bill...   \n",
       "17  What mechanisms are needed for ethical use of ...   \n",
       "18  What's the specific requirement for private en...   \n",
       "19  How are environmental impacts and sustainabili...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [ \\n \\n  \\n \\n \\nNOTICE & \\nEXPLANATION \\nWHY ...   \n",
       "1   [ ­­­­­­­\\nALGORITHMIC DISCRIMINATION Protecti...   \n",
       "2   [ \\n \\n \\n \\n \\n \\n \\nDATA PRIVACY \\nWHAT SHOU...   \n",
       "3   [ \\n16 \\nGOVERN 1.5: Ongoing monitoring and pe...   \n",
       "4   [ \\nNIST Trustworthy and Responsible AI  \\nNIS...   \n",
       "5   [ \\n \\n  \\n \\n \\nNOTICE & \\nEXPLANATION \\nWHY ...   \n",
       "6   [ \\n36 \\nMEASURE 2.11: Fairness and bias – as ...   \n",
       "7   [  \\n  \\nAPPENDIX\\nPanel 3: Equal Opportunitie...   \n",
       "8   [ \\n \\n \\nAbout AI at NIST: The National Insti...   \n",
       "9   [ \\n \\n  \\nWHY THIS PRINCIPLE IS IMPORTANT\\nTh...   \n",
       "10  [ \\n53 \\nDocumenting, reporting, and sharing i...   \n",
       "11  [ \\n \\n \\n \\n \\n \\n \\nWHAT SHOULD BE EXPECTED ...   \n",
       "12  [ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n...   \n",
       "13  [ \\n11 \\nvalue chain (e.g., data inputs, proce...   \n",
       "14  [ \\n6 \\n2.2. Confabulation \\n“Confabulation” r...   \n",
       "15  [ \\n48 \\n• Data protection \\n• Data retention ...   \n",
       "16  [ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAbo...   \n",
       "17  [ \\n \\n \\n \\n \\n \\n \\nHUMAN ALTERNATIVES, \\nCO...   \n",
       "18  [ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n...   \n",
       "19  [ \\n37 \\nMS-2.11-005 \\nAssess the proportion o...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   Providing notice and explanation is important ...   \n",
       "1   Designers, developers, and deployers of automa...   \n",
       "2   Reporting for automated systems used in sensit...   \n",
       "3   Mechanisms for inventorying AI systems include...   \n",
       "4                                       I don't know.   \n",
       "5   It is important for both police and the public...   \n",
       "6   Continuous monitoring of GAI system impacts ca...   \n",
       "7   The state-wide biometrics moratorium was spark...   \n",
       "8   According to the National Institute of Standar...   \n",
       "9   Advertisement delivery systems reinforce racia...   \n",
       "10                                      I don't know.   \n",
       "11  A fair and accessible algorithmic impact asses...   \n",
       "12  Ballot curing laws protect voter rights by req...   \n",
       "13  The context provided mentions the importance o...   \n",
       "14  Confabulated content in GAI systems poses sign...   \n",
       "15                                      I don't know.   \n",
       "16  The OSTP collected input for the AI Bill of Ri...   \n",
       "17  The mechanisms needed for ethical use of autom...   \n",
       "18  Private entities in Illinois are required to p...   \n",
       "19  Environmental impact and sustainability of AI ...   \n",
       "\n",
       "                                         ground_truth  faithfulness  \\\n",
       "0   Providing notice and explanations in the conte...      1.000000   \n",
       "1   Designers, developers, and deployers of automa...      1.000000   \n",
       "2   Reporting for automated systems used in sensit...      1.000000   \n",
       "3   Mechanisms for inventorying AI systems include...      1.000000   \n",
       "4   The National Institute of Standards and Techno...      0.000000   \n",
       "5   Both police and the public deserve to understa...      1.000000   \n",
       "6   Continuous monitoring of GAI system impacts ca...      1.000000   \n",
       "7   A state-wide biometrics moratorium was sparked...      1.000000   \n",
       "8   Towards a Standard for Identifying and Managin...      0.769231   \n",
       "9   Advertisement delivery systems reinforce racia...      1.000000   \n",
       "10  The answer to given question is not present in...      0.000000   \n",
       "11  The answer to given question is not present in...      1.000000   \n",
       "12  Ballot curing laws in at least 24 states provi...      1.000000   \n",
       "13  Conduct appropriate diligence on training data...      0.714286   \n",
       "14  Risks from confabulated content in GAI systems...      1.000000   \n",
       "15  Regular adversarial testing is important for G...      0.000000   \n",
       "16  The White House Office of Science and Technolo...      1.000000   \n",
       "17  The expectations for automated systems in sens...      1.000000   \n",
       "18  Private entities in Illinois are required to p...      0.857143   \n",
       "19  Environmental impacts and sustainability of AI...      0.777778   \n",
       "\n",
       "    answer_relevancy  context_recall  context_precision  answer_correctness  \n",
       "0           0.996779             1.0           1.000000            0.747138  \n",
       "1           0.976883             1.0           1.000000            0.653547  \n",
       "2           1.000000             1.0           1.000000            0.266613  \n",
       "3           1.000000             1.0           1.000000            0.996998  \n",
       "4           0.000000             0.0           0.416667            0.179853  \n",
       "5           0.995294             0.5           1.000000            0.784810  \n",
       "6           0.898330             1.0           1.000000            0.761270  \n",
       "7           0.928142             1.0           1.000000            0.529175  \n",
       "8           0.951132             1.0           0.500000            0.227159  \n",
       "9           1.000000             1.0           0.916667            0.997382  \n",
       "10          0.000000             1.0           0.000000            0.195204  \n",
       "11          0.971865             1.0           0.000000            0.172466  \n",
       "12          0.978773             1.0           1.000000            0.708202  \n",
       "13          0.928923             0.6           1.000000            0.683369  \n",
       "14          0.972048             1.0           1.000000            0.994807  \n",
       "15          0.000000             1.0           1.000000            0.179221  \n",
       "16          1.000000             1.0           1.000000            0.991035  \n",
       "17          0.971665             1.0           1.000000            0.737533  \n",
       "18          0.956440             1.0           1.000000            0.947555  \n",
       "19          0.954457             1.0           0.805556            0.893801  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = results.to_pandas()\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-experimental 0.0.64 requires langchain-community<0.3.0,>=0.2.10, but you have langchain-community 0.3.0 which is incompatible.\n",
      "langchain-experimental 0.0.64 requires langchain-core<0.3.0,>=0.2.27, but you have langchain-core 0.3.2 which is incompatible.\n",
      "langgraph 0.2.16 requires langchain-core<0.3,>=0.2.27, but you have langchain-core 0.3.2 which is incompatible.\n",
      "ragas 0.1.20 requires langchain-core<0.3, but you have langchain-core 0.3.2 which is incompatible.\n",
      "langgraph-checkpoint 1.0.6 requires langchain-core<0.3,>=0.2.22, but you have langchain-core 0.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "!pip install -qU langchain_openai langchain_huggingface langchain_core langchain langchain_community langchain-text-splitters\n",
    "!pip install -qU faiss-cpu unstructured==0.15.7 python-pptx==1.0.2 nltk==3.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 750,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "training_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "id_set = set()\n",
    "\n",
    "for document in training_documents:\n",
    "  id = str(uuid.uuid4())\n",
    "  while id in id_set:\n",
    "    id = uuid.uuid4()\n",
    "  id_set.add(id)\n",
    "  document.metadata[\"id\"] = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_split_documents = training_documents[:300]\n",
    "val_split_documents = training_documents[300:350]\n",
    "test_split_documents = training_documents[350:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "qa_chat_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_prompt = \"\"\"\\\n",
    "Given the following context, you must generate questions based on only the provided context.\n",
    "\n",
    "You are to generate {n_questions} questions which should be provided in the following format:\n",
    "\n",
    "1. QUESTION #1\n",
    "2. QUESTION #2\n",
    "...\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)\n",
    "\n",
    "question_generation_chain = qa_prompt_template | qa_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def create_questions(documents, n_questions):\n",
    "\n",
    "  questions = {}\n",
    "\n",
    "  relevant_docs = {}\n",
    "\n",
    "  for document in tqdm.tqdm(documents):\n",
    "\n",
    "    document_content = {\"context\" : document.page_content, \"questions\" : []}\n",
    "\n",
    "    questions_generated = question_generation_chain.invoke({\"context\": document.page_content, \"n_questions\": n_questions})\n",
    "\n",
    "    for question in questions_generated.content.split(\"\\n\"):\n",
    "\n",
    "      question_id = str(uuid.uuid4())\n",
    "\n",
    "      questions[question_id] = \"\".join(question.split(\".\")[1:]).strip()\n",
    "\n",
    "      relevant_docs[question_id] = [document.metadata[\"id\"]]\n",
    "\n",
    "  return questions, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [06:01<00:00,  1.20s/it]\n",
      "100%|██████████| 50/50 [00:59<00:00,  1.18s/it]\n",
      "100%|██████████| 50/50 [00:56<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "training_questions, training_relevant_contexts = create_questions(training_split_documents, 2)\n",
    "val_questions, val_relevant_contexts = create_questions(val_split_documents, 2)\n",
    "test_questions, test_relevant_contexts = create_questions(test_split_documents, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "training_corpus = {train_item.metadata[\"id\"] : train_item.page_content for train_item in training_split_documents}\n",
    "\n",
    "train_dataset = {\n",
    "    \"questions\" : training_questions,\n",
    "    \"relevant_contexts\" : training_relevant_contexts,\n",
    "    \"corpus\" : training_corpus\n",
    "}\n",
    "\n",
    "with open(\"training_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(train_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_corpus = {val_item.metadata[\"id\"] : val_item.page_content for val_item in val_split_documents}\n",
    "\n",
    "val_dataset = {\n",
    "    \"questions\" : val_questions,\n",
    "    \"relevant_contexts\" : val_relevant_contexts,\n",
    "    \"corpus\" : val_corpus\n",
    "}\n",
    "\n",
    "with open(\"val_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(val_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "train_corpus = {test_item.metadata[\"id\"] : test_item.page_content for test_item in test_split_documents}\n",
    "\n",
    "test_dataset = {\n",
    "    \"questions\" : test_questions,\n",
    "    \"relevant_contexts\" : test_relevant_contexts,\n",
    "    \"corpus\" : train_corpus\n",
    "}\n",
    "\n",
    "with open(\"test_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "trained_docs_vectorstore = Qdrant.from_documents(\n",
    "    documents=training_documents, # RecursiveCharacterTextSplitter\n",
    "    embedding=embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"Fine Tuned Implications of AI- RecursiveCharacterTextSplitter\"\n",
    ")\n",
    "\n",
    "finetune_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "finetune_vectorstore = Qdrant.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=finetune_embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"Fine Tuned Implications of AI- TE3\"\n",
    ")\n",
    "\n",
    "finetune_retriever = vectorstore.as_retriever()\n",
    "finetune_retriever = finetune_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'A concern of AI is the potential for outputs to be erroneous, which may lead to ill-founded decision-making or amplify harmful biases. Additionally, there are risks associated with human-AI configuration, such as inappropriate anthropomorphizing of AI systems, algorithmic aversion, automation bias, over-reliance, or emotional entanglement with AI systems. Furthermore, there is a lowered barrier to entry for generating and supporting the exchange of content that may not distinguish fact from opinion or fiction, which could be leveraged for large-scale disinformation campaigns.', 'context': [Document(metadata={'source': 'data/NIST.AI.600-1.pdf', 'file_path': 'data/NIST.AI.600-1.pdf', 'page': 7, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'id': 'e033f4a2-32c6-4bda-966f-c47dfb7eae07', '_id': '81ae4bad299b4899b2b35ed5ceb5dc6f', '_collection_name': 'Fine Tuned Implications of AI'}, page_content='outputs, which may be erroneous, lead to ill-founded decision-making, or amplify harmful \\nbiases.  \\n7. Human-AI Conﬁguration: Arrangements of or interactions between a human and an AI system \\nwhich can result in the human inappropriately anthropomorphizing GAI systems or experiencing \\nalgorithmic aversion, automation bias, over-reliance, or emotional entanglement with GAI \\nsystems. \\n8. Information Integrity: Lowered barrier to entry to generate and support the exchange and \\nconsumption of content which may not distinguish fact from opinion or ﬁction or acknowledge \\nuncertainties, or could be leveraged for large-scale dis- and mis-information campaigns.'), Document(metadata={'source': 'data/NIST.AI.600-1.pdf', 'file_path': 'data/NIST.AI.600-1.pdf', 'page': 55, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'id': '53f5e9cb-d09e-4b8c-b846-c5d5dfc45d1e', '_id': '3b6dbc3e6636463781f25fb0c45ced32', '_collection_name': 'Fine Tuned Implications of AI'}, page_content='Overview \\nAI incidents can be deﬁned as an “event, circumstance, or series of events where the development, use, \\nor malfunction of one or more AI systems directly or indirectly contributes to one of the following harms: \\ninjury or harm to the health of a person or groups of people (including psychological harms and harms to \\nmental health); disruption of the management and operation of critical infrastructure; violations of \\nhuman rights or a breach of obligations under applicable law intended to protect fundamental, labor, \\nand intellectual property rights; or harm to property, communities, or the environment.” AI incidents can \\noccur in the aggregate (i.e., for systemic discrimination) or acutely (i.e., for one individual).'), Document(metadata={'source': 'data/NIST.AI.600-1.pdf', 'file_path': 'data/NIST.AI.600-1.pdf', 'page': 31, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'id': '78e50ee6-2e1d-412c-a2ed-ea4d71127e01', '_id': '07cba35ef88540acbcea92ef5949814e', '_collection_name': 'Fine Tuned Implications of AI'}, page_content='impacts. \\nHuman-AI Conﬁguration; Value \\nChain and Component Integration \\nAI Actor Tasks: AI Deployment, AI Design, AI Impact Assessment, Aﬀected Individuals and Communities, Domain Experts, End-\\nUsers, Human Factors, Operation and Monitoring  \\n \\nMEASURE 1.1: Approaches and metrics for measurement of AI risks enumerated during the MAP function are selected for \\nimplementation starting with the most signiﬁcant AI risks. The risks or trustworthiness characteristics that will not – or cannot – be \\nmeasured are properly documented. \\nAction ID \\nSuggested Action \\nGAI Risks \\nMS-1.1-001 Employ methods to trace the origin and modiﬁcations of digital content. \\nInformation Integrity \\nMS-1.1-002'), Document(metadata={'source': 'data/NIST.AI.600-1.pdf', 'file_path': 'data/NIST.AI.600-1.pdf', 'page': 6, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'id': '36a2f8c8-bda3-4310-8fd7-58850adc957b', '_id': '2f1fa395618c4d37a8f6a66229312a74', '_collection_name': 'Fine Tuned Implications of AI'}, page_content='3 \\nthe abuse, misuse, and unsafe repurposing by humans (adversarial or not), and others result \\nfrom interactions between a human and an AI system.  \\n• \\nTime scale: GAI risks may materialize abruptly or across extended periods. Examples include \\nimmediate (and/or prolonged) emotional harm and potential risks to physical safety due to the \\ndistribution of harmful deepfake images, or the long-term eﬀect of disinformation on societal \\ntrust in public institutions. \\nThe presence of risks and where they fall along the dimensions above will vary depending on the \\ncharacteristics of the GAI model, system, or use case at hand. These characteristics include but are not'), Document(metadata={'source': 'data/NIST.AI.600-1.pdf', 'file_path': 'data/NIST.AI.600-1.pdf', 'page': 44, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'id': '5194e319-83ae-4d92-85c0-2abc8587e71e', '_id': '1aeb960ac9b54a3ea86ff28a464c5643', '_collection_name': 'Fine Tuned Implications of AI'}, page_content='41 \\nMG-2.2-006 \\nUse feedback from internal and external AI Actors, users, individuals, and \\ncommunities, to assess impact of AI-generated content. \\nHuman-AI Conﬁguration \\nMG-2.2-007 \\nUse real-time auditing tools where they can be demonstrated to aid in the \\ntracking and validation of the lineage and authenticity of AI-generated data. \\nInformation Integrity \\nMG-2.2-008 \\nUse structured feedback mechanisms to solicit and capture user input about AI-\\ngenerated content to detect subtle shifts in quality or alignment with \\ncommunity and societal values. \\nHuman-AI Conﬁguration; Harmful \\nBias and Homogenization \\nMG-2.2-009 \\nConsider opportunities to responsibly use synthetic data and other privacy'), Document(metadata={'source': 'data/NIST.AI.600-1.pdf', 'file_path': 'data/NIST.AI.600-1.pdf', 'page': 23, 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile', 'author': 'National Institute of Standards and Technology', 'subject': '', 'keywords': '', 'creator': 'Acrobat PDFMaker 24 for Word', 'producer': 'Adobe PDF Library 24.2.159', 'creationDate': \"D:20240805141702-04'00'\", 'modDate': \"D:20240805143048-04'00'\", 'trapped': '', 'id': '3a52610a-fce9-40be-a444-e37374399506', '_id': 'c6628a953f7c4fe39724b584df4027fa', '_collection_name': 'Fine Tuned Implications of AI'}, page_content='Human-AI Conﬁguration; Harmful \\nBias and Homogenization \\nGV-5.1-002 \\nDocument interactions with GAI systems to users prior to interactive activities, \\nparticularly in contexts involving more signiﬁcant risks.  \\nHuman-AI Conﬁguration; \\nConfabulation \\nAI Actor Tasks: AI Design, AI Impact Assessment, Aﬀected Individuals and Communities, Governance and Oversight \\n \\nGOVERN 6.1: Policies and procedures are in place that address AI risks associated with third-party entities, including risks of \\ninfringement of a third-party’s intellectual property or other rights. \\nAction ID \\nSuggested Action \\nGAI Risks \\nGV-6.1-001 Categorize diﬀerent types of GAI content with associated third-party rights (e.g., \\ncopyright, intellectual property, data privacy).')]}\n"
     ]
    }
   ],
   "source": [
    "finetune_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | finetune_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "\n",
    "fine_tuned_result = finetune_rag_chain.invoke({\"question\" : \"What is a concern of AI?\"})\n",
    "print(fine_tuned_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-huggingface 0.1.0 requires langchain-core<0.4,>=0.3.0, but you have langchain-core 0.2.41 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7017f3ed77de4506863f373d15f0275a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Risk assessment', 'Public inspection', 'Trade secrets', 'Discovery in criminal matter']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Principles into practice', 'Real-life examples', 'Combat discrimination', 'Mortgage lending', 'Nationwide initiative']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Americans with Disabilities Act', 'Title I', 'Disparity assessments', 'Healthcare algorithm', \"Black patients' healthcare access\"]}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Automated systems', 'Technical standards', 'Algorithmic discrimination', 'Anti-discrimination law', 'Proactive technical steps']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Protection Bureau', 'Prudential regulators', 'Automated Valuation Models', 'Equal Employment Opportunity Commission', 'Department of Justice']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Sociodemographic variables', \"Algorithm's output\", \"Patient's race or ethnicity\", 'Race-based health inequities', 'Algorithmic Discrimination Protections']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Legal protections', 'Equity for underserved communities', 'Algorithmic discrimination', 'Proactive equity assessments', 'Automated systems']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Research and development', 'Acquisition review', 'Potential discrimination', 'Equity effects', 'Underserved communities']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Algorithmic discrimination', 'Automated systems', 'Unjustified different treatment', 'Protected classifications', 'Legal protections']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['White patients', 'Landmark study', 'Algorithmic Bias Safeguards', 'Chronic health conditions', 'Emergency visits']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Compliance', 'Transparency and validity requirements', 'Algorithmic pretrial risk assessments', 'Civil rights groups', 'Idaho Code Section 19-1910']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Automated systems', 'Algorithmic discrimination', 'Equitable design', 'Disparity testing', 'Organizational oversight']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Accessibility criteria', 'Technology design processes', 'Section 508 regulations', 'Web Content Accessibility Guidelines', 'Managing Bias']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['LGBTQI+ persons', 'Older adults', 'Persons with disabilities', 'Rural areas', 'Persistent poverty or inequality']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Performed', 'Made public', 'Confirm protections']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Prohibited by law', 'Proactive testing', 'Identify proxies', 'Algorithmic discrimination', 'System monitoring']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Automated systems', 'Technical standards', 'Accessibility', 'People with disabilities', 'User experience research']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['User experience', 'Design decisions', 'Privacy invasive', 'Consent requests', 'Data collection']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Artificial Intelligence', 'Bias in AI', 'Systemic, statistical, and human bias', 'Mitigating bias challenges', 'Algorithmic Discrimination Protections']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Algorithmic Discrimination Protections', 'Automated systems', 'Inequitable outcomes', 'Facial recognition technology', 'Discriminatory decisions']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Severity of certain diseases in Black Americans', 'Discriminatory practices in AI and automated systems', 'Algorithmic discrimination protections', 'Bias testing in product quality assessment', 'Federal government agencies taking steps']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Mitigate biases', 'Guarding against proxies', 'Algorithmic discrimination', 'Demographic information', 'Automated system']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Disparity assessment', 'Automated systems testing', 'Demographics inclusivity', 'Protected classifications', 'Performance measures']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Data pertaining to youth', 'Sensitive domains', 'Ethical review', 'Unchecked surveillance', 'Surveillance technologies']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Abusive data practices', 'Privacy violations', 'Data collection', 'Permission', 'Privacy by design safeguards']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Automated systems', 'Fair treatment', 'Equity', 'Underserved communities', 'Proactive protections']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Access to reporting', 'Data decisions', 'Surveillance technologies', 'Rights', 'Data privacy']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Disparity assessment', 'Automated system', 'Privacy protections', 'Disparity mitigation', 'Identified groups']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Automated systems', 'Algorithmic discrimination', 'Guardrails protecting the public', 'Mitigation of biases', 'Impact assessments']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Disparity mitigation', 'Algorithmic discrimination', 'Automated system evaluation', 'Equity goals', 'Data input choices']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Data privacy', 'Cross-cutting principle', 'Surveillance and data collection', 'Business models', 'Automated systems']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Large disparities by race', 'Risk scores', 'Guide students', 'Math and science subjects', 'Risk assessment tool']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['American public', 'Government agencies', 'Surveillance capabilities', 'Private data collection', 'Data brokers']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Historically Black College or University', 'Higher loan prices', 'Refinancing student loan', 'Hiring tool discrimination', 'Predictive model for student dropout']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Violent recidivism tools', 'Department of Justice', 'Reduce disparities', 'Publicly released report']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Target measure', 'Unobservable targets', 'Inappropriate use of proxies', 'Algorithmic discrimination', 'Equity goals']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Automated system', 'Algorithmic discrimination', 'Demographic information', 'Riskier systems', 'Disparity mitigation']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Inaccurate and faulty data', 'Surveillance technologies', 'Mental health harms', 'Data brokers', 'Data harvesting']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Fallback procedures', 'Equity standards', 'Prior mechanisms', 'Algorithmic discrimination protections']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Data public', 'Encourage researchers', 'Identifying problem', 'Sexualized content', 'Racial and gender stereotypes']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['National AI Research Institutes', 'Safe and trustworthy AI algorithms', 'Cyber Physical Systems program', 'Secure and Trustworthy Cyberspace program', 'Formal Methods in the Field program']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Safe and effective systems', 'Real-life examples', 'Ethical use of AI systems', 'Department of Energy AI Advancement Council', 'Department of Defense Artificial Intelligence']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Automated sentiment analyzer', 'Technology platforms', 'Biased against Jews and gay people', 'Positive or negative sentiment', 'Preemptive blocking of social media comments']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Equity assessments', 'Accessibility designs', 'Disparity testing', 'Mitigation implementation', 'Algorithmic impact assessment']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['AI technologies', 'Testing and evaluation', 'Winter of 2022-23']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Enforcement or national security restrictions', 'Individual privacy', 'Evaluation data access needs', 'Policy-based and/or technological innovations', 'Algorithmic impact assessment']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Ethical Principles', 'Responsible Artificial Intelligence', 'National security and defense activities', 'U.S. Intelligence Community', 'AI Ethics Framework']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Automated systems', 'Technical standards', 'Algorithmic discrimination', 'Independent evaluation', 'Public sector uses']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['National Disabled Law Students Association', 'Remote proctoring AI systems', 'Patients with high needs for healthcare', 'Black patients', 'Healthcare clinical algorithms']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Cashier ads to women', 'Jobs with taxi companies to primarily Black people', 'Body scanners at airport checkpoints', 'Gender identity', 'Transgender travelers']}\n",
      "[ragas.testset.docstore.INFO] Document [ID: 3cb78bb1-2555-4964-9392-6c1318fe248e] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 3cb78bb1-2555-4964-9392-6c1318fe248e] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 9a7731a3-70bc-4bc0-836b-4cdb3fb57981] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 9a7731a3-70bc-4bc0-836b-4cdb3fb57981] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 334963b8-8030-42b3-889e-de3c31e1ade4] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 334963b8-8030-42b3-889e-de3c31e1ade4] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: cee5f89a-2ca6-4da8-91b2-a2bf33dca2aa] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: cee5f89a-2ca6-4da8-91b2-a2bf33dca2aa] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 05bda18a-17ca-43ad-a4c9-d1d0ccfa7c79] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 05bda18a-17ca-43ad-a4c9-d1d0ccfa7c79] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: eab6ec3a-f471-4fb8-9eba-d446e2fcf5e8] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: eab6ec3a-f471-4fb8-9eba-d446e2fcf5e8] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 943d04b1-3612-4017-b551-851befdc5ab2] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 943d04b1-3612-4017-b551-851befdc5ab2] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 5542e2b3-fbf7-47b1-a513-fdd8933bdbb9] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 5542e2b3-fbf7-47b1-a513-fdd8933bdbb9] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 657fc26b-a6f3-4846-9580-7478a6aa5519] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 657fc26b-a6f3-4846-9580-7478a6aa5519] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 74404bf8-9da0-4ca0-830f-839f8b5dac77] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 74404bf8-9da0-4ca0-830f-839f8b5dac77] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: f8d8c729-2011-4cb8-b55a-6d89499a04e7] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: f8d8c729-2011-4cb8-b55a-6d89499a04e7] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 2382c153-6e30-4311-9ab7-adc64ddc3b86] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 2382c153-6e30-4311-9ab7-adc64ddc3b86] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: f1bf4f18-8a51-4d4d-8eaa-a099e5ab5a6e] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: f1bf4f18-8a51-4d4d-8eaa-a099e5ab5a6e] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 9292c959-03f0-4116-98d9-b9eada4c8708] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 9292c959-03f0-4116-98d9-b9eada4c8708] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 697f0954-1d34-42cd-bd7d-da200cdd8ecf] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 697f0954-1d34-42cd-bd7d-da200cdd8ecf] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: df3c2104-18c8-441e-ad73-d7ca1f36580b] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: df3c2104-18c8-441e-ad73-d7ca1f36580b] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: fdfe585b-cba0-4f81-9365-90c71acd1342] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: fdfe585b-cba0-4f81-9365-90c71acd1342] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: b759b40f-5289-4c60-b6c6-56f262b077fb] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: b759b40f-5289-4c60-b6c6-56f262b077fb] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: d8935a7f-8304-488f-8939-a25934aba778] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: d8935a7f-8304-488f-8939-a25934aba778] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 55104141-647a-43bb-8bb6-ff2c36ebd09c] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 55104141-647a-43bb-8bb6-ff2c36ebd09c] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 3ce058e2-610c-4b7d-bba9-eaf039c2f7cf] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 3ce058e2-610c-4b7d-bba9-eaf039c2f7cf] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: bba41976-6ee5-458d-ac76-e9f7bdbd949a] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: bba41976-6ee5-458d-ac76-e9f7bdbd949a] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 2218822e-d9cb-43f8-939a-a9dfe6874119] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 2218822e-d9cb-43f8-939a-a9dfe6874119] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 85dbec5a-624b-451d-9f84-b14d23ffe433] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 85dbec5a-624b-451d-9f84-b14d23ffe433] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 6bf78072-8973-4d87-a079-4fe2d4b7e86e] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 6bf78072-8973-4d87-a079-4fe2d4b7e86e] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: e84cd1ce-e0a4-4b62-90de-ced67edd7329] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: e84cd1ce-e0a4-4b62-90de-ced67edd7329] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: cb4404ed-9b91-4c5e-af63-c279518d6221] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: cb4404ed-9b91-4c5e-af63-c279518d6221] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: ce5b8677-889c-48b5-910e-ec5357cd5243] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: ce5b8677-889c-48b5-910e-ec5357cd5243] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: fdad2099-3098-42e9-b392-51dc27cc5108] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: fdad2099-3098-42e9-b392-51dc27cc5108] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 32ba26d4-f669-4947-a3c3-de191d89943a] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 32ba26d4-f669-4947-a3c3-de191d89943a] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 58b5d6e9-b64c-4705-a460-6f1b6645e87f] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 58b5d6e9-b64c-4705-a460-6f1b6645e87f] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 098b6a13-87e4-418a-a419-f00e86abcf49] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 098b6a13-87e4-418a-a419-f00e86abcf49] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 4610aa3e-c030-4db1-8aab-b87ae7ef8865] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 4610aa3e-c030-4db1-8aab-b87ae7ef8865] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 527bb1a9-8921-47f1-9601-0a561bdcc64d] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 527bb1a9-8921-47f1-9601-0a561bdcc64d] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 39cb8c89-7243-44c0-a725-66baac0c2600] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 39cb8c89-7243-44c0-a725-66baac0c2600] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 17cc19c8-999f-4964-92d6-3f7e7cdc99d9] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 17cc19c8-999f-4964-92d6-3f7e7cdc99d9] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 1011cb59-fe80-412d-a1e3-88d4ff6daed8] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 1011cb59-fe80-412d-a1e3-88d4ff6daed8] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 706e16f9-2d02-4c27-a669-69271397c21b] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 706e16f9-2d02-4c27-a669-69271397c21b] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 68ec18ab-8ffa-41d0-8840-3e103208469f] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 68ec18ab-8ffa-41d0-8840-3e103208469f] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 15d682af-614c-45b3-8fe6-9e3b5a6a674a] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 15d682af-614c-45b3-8fe6-9e3b5a6a674a] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: d3830004-4fe9-4b72-b1b1-75305e3730f5] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: d3830004-4fe9-4b72-b1b1-75305e3730f5] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 39738073-680c-4616-9d81-3c729676ccf4] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 39738073-680c-4616-9d81-3c729676ccf4] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 62e1c28a-fae2-4e57-aae5-3bc01418883b] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 62e1c28a-fae2-4e57-aae5-3bc01418883b] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: a8761fe8-ebc7-4ce0-8dc3-09e79bc45198] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: a8761fe8-ebc7-4ce0-8dc3-09e79bc45198] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 425e1d05-e9d0-4d9a-8ff3-6ee5e7c32b42] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 425e1d05-e9d0-4d9a-8ff3-6ee5e7c32b42] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 71d83229-bf60-4a24-9095-6a9813c618d1] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 71d83229-bf60-4a24-9095-6a9813c618d1] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 291b80e1-a196-473e-bd45-8b8335ac5cbd] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 291b80e1-a196-473e-bd45-8b8335ac5cbd] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 9b24298d-8f79-4bed-9a77-179c26697f3c] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 9b24298d-8f79-4bed-9a77-179c26697f3c] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: aec3588c-fa3a-4595-93f7-a82063a2162f] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: aec3588c-fa3a-4595-93f7-a82063a2162f] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 5e4b7070-007e-4ab4-891a-ccb27a92336b] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 5e4b7070-007e-4ab4-891a-ccb27a92336b] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.WARNING] Filename and doc_id are the same for all nodes.\n",
      "[ragas.testset.docstore.INFO] Document [ID: 3cb78bb1-2555-4964-9392-6c1318fe248e] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 9a7731a3-70bc-4bc0-836b-4cdb3fb57981] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 9a7731a3-70bc-4bc0-836b-4cdb3fb57981] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 334963b8-8030-42b3-889e-de3c31e1ade4] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 334963b8-8030-42b3-889e-de3c31e1ade4] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: cee5f89a-2ca6-4da8-91b2-a2bf33dca2aa] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: cee5f89a-2ca6-4da8-91b2-a2bf33dca2aa] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 05bda18a-17ca-43ad-a4c9-d1d0ccfa7c79] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 05bda18a-17ca-43ad-a4c9-d1d0ccfa7c79] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: eab6ec3a-f471-4fb8-9eba-d446e2fcf5e8] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: eab6ec3a-f471-4fb8-9eba-d446e2fcf5e8] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 943d04b1-3612-4017-b551-851befdc5ab2] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 943d04b1-3612-4017-b551-851befdc5ab2] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 5542e2b3-fbf7-47b1-a513-fdd8933bdbb9] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 5542e2b3-fbf7-47b1-a513-fdd8933bdbb9] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 657fc26b-a6f3-4846-9580-7478a6aa5519] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 657fc26b-a6f3-4846-9580-7478a6aa5519] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 74404bf8-9da0-4ca0-830f-839f8b5dac77] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 74404bf8-9da0-4ca0-830f-839f8b5dac77] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: f8d8c729-2011-4cb8-b55a-6d89499a04e7] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: f8d8c729-2011-4cb8-b55a-6d89499a04e7] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 2382c153-6e30-4311-9ab7-adc64ddc3b86] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 2382c153-6e30-4311-9ab7-adc64ddc3b86] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: f1bf4f18-8a51-4d4d-8eaa-a099e5ab5a6e] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: f1bf4f18-8a51-4d4d-8eaa-a099e5ab5a6e] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 9292c959-03f0-4116-98d9-b9eada4c8708] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 9292c959-03f0-4116-98d9-b9eada4c8708] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 697f0954-1d34-42cd-bd7d-da200cdd8ecf] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 697f0954-1d34-42cd-bd7d-da200cdd8ecf] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: df3c2104-18c8-441e-ad73-d7ca1f36580b] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: df3c2104-18c8-441e-ad73-d7ca1f36580b] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: fdfe585b-cba0-4f81-9365-90c71acd1342] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: fdfe585b-cba0-4f81-9365-90c71acd1342] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: b759b40f-5289-4c60-b6c6-56f262b077fb] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: b759b40f-5289-4c60-b6c6-56f262b077fb] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: d8935a7f-8304-488f-8939-a25934aba778] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: d8935a7f-8304-488f-8939-a25934aba778] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 55104141-647a-43bb-8bb6-ff2c36ebd09c] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 55104141-647a-43bb-8bb6-ff2c36ebd09c] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 3ce058e2-610c-4b7d-bba9-eaf039c2f7cf] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 3ce058e2-610c-4b7d-bba9-eaf039c2f7cf] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: bba41976-6ee5-458d-ac76-e9f7bdbd949a] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: bba41976-6ee5-458d-ac76-e9f7bdbd949a] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 2218822e-d9cb-43f8-939a-a9dfe6874119] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 2218822e-d9cb-43f8-939a-a9dfe6874119] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 85dbec5a-624b-451d-9f84-b14d23ffe433] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 85dbec5a-624b-451d-9f84-b14d23ffe433] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 6bf78072-8973-4d87-a079-4fe2d4b7e86e] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 6bf78072-8973-4d87-a079-4fe2d4b7e86e] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: e84cd1ce-e0a4-4b62-90de-ced67edd7329] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: e84cd1ce-e0a4-4b62-90de-ced67edd7329] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: cb4404ed-9b91-4c5e-af63-c279518d6221] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: cb4404ed-9b91-4c5e-af63-c279518d6221] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: ce5b8677-889c-48b5-910e-ec5357cd5243] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: ce5b8677-889c-48b5-910e-ec5357cd5243] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: fdad2099-3098-42e9-b392-51dc27cc5108] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: fdad2099-3098-42e9-b392-51dc27cc5108] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 32ba26d4-f669-4947-a3c3-de191d89943a] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 32ba26d4-f669-4947-a3c3-de191d89943a] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 58b5d6e9-b64c-4705-a460-6f1b6645e87f] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 58b5d6e9-b64c-4705-a460-6f1b6645e87f] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 098b6a13-87e4-418a-a419-f00e86abcf49] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 098b6a13-87e4-418a-a419-f00e86abcf49] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 4610aa3e-c030-4db1-8aab-b87ae7ef8865] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 4610aa3e-c030-4db1-8aab-b87ae7ef8865] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 527bb1a9-8921-47f1-9601-0a561bdcc64d] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 527bb1a9-8921-47f1-9601-0a561bdcc64d] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 39cb8c89-7243-44c0-a725-66baac0c2600] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 39cb8c89-7243-44c0-a725-66baac0c2600] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 17cc19c8-999f-4964-92d6-3f7e7cdc99d9] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 17cc19c8-999f-4964-92d6-3f7e7cdc99d9] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 1011cb59-fe80-412d-a1e3-88d4ff6daed8] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 1011cb59-fe80-412d-a1e3-88d4ff6daed8] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 706e16f9-2d02-4c27-a669-69271397c21b] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 706e16f9-2d02-4c27-a669-69271397c21b] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 68ec18ab-8ffa-41d0-8840-3e103208469f] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 68ec18ab-8ffa-41d0-8840-3e103208469f] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 15d682af-614c-45b3-8fe6-9e3b5a6a674a] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 15d682af-614c-45b3-8fe6-9e3b5a6a674a] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: d3830004-4fe9-4b72-b1b1-75305e3730f5] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: d3830004-4fe9-4b72-b1b1-75305e3730f5] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 39738073-680c-4616-9d81-3c729676ccf4] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 39738073-680c-4616-9d81-3c729676ccf4] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 62e1c28a-fae2-4e57-aae5-3bc01418883b] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 62e1c28a-fae2-4e57-aae5-3bc01418883b] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: a8761fe8-ebc7-4ce0-8dc3-09e79bc45198] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: a8761fe8-ebc7-4ce0-8dc3-09e79bc45198] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 425e1d05-e9d0-4d9a-8ff3-6ee5e7c32b42] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 425e1d05-e9d0-4d9a-8ff3-6ee5e7c32b42] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 71d83229-bf60-4a24-9095-6a9813c618d1] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 71d83229-bf60-4a24-9095-6a9813c618d1] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 291b80e1-a196-473e-bd45-8b8335ac5cbd] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 291b80e1-a196-473e-bd45-8b8335ac5cbd] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 9b24298d-8f79-4bed-9a77-179c26697f3c] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 9b24298d-8f79-4bed-9a77-179c26697f3c] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: aec3588c-fa3a-4595-93f7-a82063a2162f] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: aec3588c-fa3a-4595-93f7-a82063a2162f] has no filename, using `doc_id` instead\n",
      "[ragas.testset.docstore.INFO] Document [ID: 5e4b7070-007e-4ab4-891a-ccb27a92336b] has no filename, using `doc_id` instead\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada1950f62364570b4777a9d9a8bbf27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Target measure', 'Unobservable targets', 'Inappropriate use of proxies', 'Algorithmic discrimination', 'Equity goals']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Americans with Disabilities Act', 'Title I', 'Disparity assessments', 'Healthcare algorithm', \"Black patients' healthcare access\"]\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Technical standards', 'Algorithmic discrimination', 'Independent evaluation', 'Public sector uses']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Cashier ads to women', 'Jobs with taxi companies to primarily Black people', 'Body scanners at airport checkpoints', 'Gender identity', 'Transgender travelers']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Americans with Disabilities Act', 'Title I', 'Disparity assessments', 'Healthcare algorithm', \"Black patients' healthcare access\"]\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 2, 'score': 1.25}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 0 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Protection Bureau', 'Prudential regulators', 'Automated Valuation Models', 'Equal Employment Opportunity Commission', 'Department of Justice']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['American public', 'Government agencies', 'Surveillance capabilities', 'Private data collection', 'Data brokers']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Algorithmic discrimination', 'Guardrails protecting the public', 'Mitigation of biases', 'Impact assessments']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['National AI Research Institutes', 'Safe and trustworthy AI algorithms', 'Cyber Physical Systems program', 'Secure and Trustworthy Cyberspace program', 'Formal Methods in the Field program']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Legal protections', 'Equity for underserved communities', 'Algorithmic discrimination', 'Proactive equity assessments', 'Automated systems']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Americans with Disabilities Act', 'Title I', 'Disparity assessments', 'Healthcare algorithm', \"Black patients' healthcare access\"]\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Legal protections', 'Equity for underserved communities', 'Algorithmic discrimination', 'Proactive equity assessments', 'Automated systems']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 2, 'structure': 3, 'relevance': 3, 'score': 2.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Accessibility criteria', 'Technology design processes', 'Section 508 regulations', 'Web Content Accessibility Guidelines', 'Managing Bias']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 2, 'structure': 3, 'relevance': 3, 'score': 2.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Accessibility criteria', 'Technology design processes', 'Section 508 regulations', 'Web Content Accessibility Guidelines', 'Managing Bias']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What role does independent evaluation play in ensuring automated systems protect against algorithmic discrimination?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How did the healthcare algorithm discriminate against Black patients in terms of access to medical care?\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['American public', 'Government agencies', 'Surveillance capabilities', 'Private data collection', 'Data brokers']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How can the inappropriate use of proxies be avoided in relation to target measures?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How are jobs with taxi companies primarily targeted towards Black people?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How has the expanding scale of private data collection impacted the American public?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the focus of the Secure and Trustworthy Cyberspace program?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How has the Department of Justice addressed the potential discrimination against job applicants and employees with disabilities related to employers' use of AI and automated systems?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How can the public be protected from algorithmic discrimination in a proactive and ongoing manner?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the significance of Title I of the Americans with Disabilities Act (ADA) in relation to healthcare access for Black patients?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How are standards, guidance, audits, and impact assessments being used to mitigate algorithmic discrimination and bias in automated systems?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How can legal protections be extended to ensure equity for underserved communities in the development and deployment of automated systems?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How have standards organizations incorporated accessibility criteria into technology design processes?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What are the Section 508 regulations and how do they impact technology design processes in the United States?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How does the Americans with Disabilities Act protect the rights of individuals in the workplace and in healthcare access?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How do government agencies use and help develop technologies that enhance and expand surveillance capabilities, impacting the American public?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the role of independent evaluation in the context of automated systems and algorithmic discrimination. It is clear in its intent, specifying the focus on independent evaluation and its relationship to protecting against algorithmic discrimination. The question is self-contained and does not rely on external references, making it understandable and answerable based on the details provided. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: What role does independent evaluation play in ensuring automated systems protect against algorithmic discrimination?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and seeks information about the actions taken by the Department of Justice regarding discrimination against individuals with disabilities in the context of AI and automated systems in employment. It is clear in its intent and does not rely on external references, making it understandable and answerable based on the details provided. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the focus of the Secure and Trustworthy Cyberspace program, which is specific and clear in its intent. It does not rely on external references or additional context, making it independent and self-contained. The question is straightforward and can be answered based on general knowledge of the program. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for an explanation of Section 508 regulations and their impact on technology design processes in the United States. It is clear in its intent, specifying both the regulations and the context of technology design. The question is self-contained and does not rely on external references, making it understandable and answerable based on the details provided. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What are the Section 508 regulations and how do they impact technology design processes in the United States?\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['User experience', 'Design decisions', 'Privacy invasive', 'Consent requests', 'Data collection']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question addresses the issue of avoiding inappropriate use of proxies in relation to target measures, which is a specific topic. However, it lacks clarity regarding what is meant by 'inappropriate use of proxies' and 'target measures'. The terms are somewhat vague and could refer to various contexts, such as statistical analysis, project management, or performance evaluation. To improve clarity and answerability, the question could specify the context in which proxies and target measures are being discussed, and provide examples of what constitutes inappropriate use. This would help in formulating a more precise and relevant response.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: How can the inappropriate use of proxies be avoided in relation to target measures?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and clear in its intent, asking about the extension of legal protections to promote equity for underserved communities in the context of automated systems. It does not rely on external references and can be understood independently. However, it could be improved by specifying which types of automated systems are being referred to (e.g., AI, machine learning, etc.) or by providing examples of the legal protections currently in place. This would help to further clarify the scope of the question and guide the response more effectively.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How can legal protections be extended to ensure equity for underserved communities in the development and deployment of automated systems?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question clearly asks about the protections offered by the Americans with Disabilities Act (ADA) regarding workplace rights and healthcare access for individuals with disabilities. It specifies the two contexts (workplace and healthcare) and seeks information on the rights protected under the ADA, making the intent clear and the question self-contained. Therefore, it can be understood and answered without needing additional context or references. The question is specific, independent, and has a clear intent.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How does the Americans with Disabilities Act protect the rights of individuals in the workplace and in healthcare access?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the impact of the expanding scale of private data collection on the American public. It is relatively clear in its intent, seeking to understand the effects of a specific phenomenon (private data collection) on a defined group (the American public). However, the question is somewhat broad and could benefit from more specificity regarding the aspects of impact being considered (e.g., privacy concerns, behavioral changes, legal implications). To improve clarity and answerability, the question could specify which dimensions of impact are of interest or provide a particular context (e.g., recent events, technological advancements) to narrow down the focus.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: How has the expanding scale of private data collection impacted the American public?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about protecting the public from algorithmic discrimination in a proactive and ongoing manner. It is clear in its intent, focusing on public protection and the specific issue of algorithmic discrimination. However, the question is somewhat broad and could benefit from more specificity regarding the context or the types of algorithms being referred to (e.g., AI, machine learning, etc.). Additionally, it could specify the stakeholders involved (e.g., government, tech companies, civil society) or the mechanisms of protection (e.g., regulations, audits, public awareness campaigns). This would enhance clarity and allow for a more focused response.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the significance of Title I of the Americans with Disabilities Act (ADA) concerning healthcare access for Black patients. It is specific in its focus on Title I of the ADA and its implications for a particular demographic group (Black patients). However, the question may require some background knowledge about the ADA, particularly Title I, and its provisions, which could make it less accessible to those unfamiliar with the legislation. To improve clarity and answerability, the question could briefly define Title I and its main provisions or specify the aspects of healthcare access being considered (e.g., discrimination, accessibility of services).', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: What is the significance of Title I of the Americans with Disabilities Act (ADA) in relation to healthcare access for Black patients?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the use of standards, guidance, audits, and impact assessments in mitigating algorithmic discrimination and bias in automated systems. It is clear in its intent and specifies the areas of interest, making it understandable. However, the question is somewhat broad and could benefit from more specificity regarding which standards or guidance are being referred to, or the context in which these measures are being applied. To improve clarity and answerability, the question could specify a particular industry, type of automated system, or recent developments in this area.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How should systems prioritize user experience and design decisions in relation to privacy concerns?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How do the Section 508 regulations impact technology design processes in the US, particularly in terms of ensuring accessibility for individuals with disabilities?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the role of government agencies in using and developing technologies for surveillance, as well as the impact on the American public. While it is specific in its focus on government agencies and surveillance technologies, it lacks clarity regarding what specific technologies or agencies are being referred to, and what aspects of the impact on the American public are of interest (e.g., privacy concerns, security benefits, social implications). To improve clarity and answerability, the question could specify particular technologies (e.g., facial recognition, data collection methods) or agencies (e.g., NSA, FBI) and clarify the type of impact being considered (e.g., legal, ethical, social).', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: How do government agencies use and help develop technologies that enhance and expand surveillance capabilities, impacting the American public?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How can legal protections ensure equity for underserved communities in the development and deployment of automated systems while proactively assessing for algorithmic discrimination and utilizing representative data?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How does the ADA safeguard the rights of individuals in workplace and healthcare, considering disparities in healthcare access and potential biases in AI systems?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The Secure and Trustworthy Cyberspace program supports research on cybersecurity and privacy enhancing technologies in automated systems.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the targeting of jobs with taxi companies towards Black people, but it lacks clarity and specificity. It is not clear what is meant by 'primarily targeted'—whether it refers to hiring practices, marketing strategies, or something else. Additionally, the question assumes a context of discrimination or specific practices without providing evidence or examples, which could lead to misunderstandings. To improve clarity and answerability, the question could specify the aspect of targeting being referred to (e.g., recruitment practices, outreach programs) and provide context or examples to clarify the intent behind the inquiry.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: How are jobs with taxi companies primarily targeted towards Black people?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the discriminatory practices of a healthcare algorithm against Black patients regarding access to medical care. It is specific in its focus on a particular demographic (Black patients) and the context of healthcare algorithms, making the intent clear. However, the question could benefit from additional context regarding which specific algorithm is being referred to, as there are many healthcare algorithms in use. Providing details about the algorithm or the study that highlights this discrimination would enhance clarity and answerability. For example, specifying the name of the algorithm or the research findings that illustrate the discrimination would make the question more self-contained.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: How did the healthcare algorithm discriminate against Black patients in terms of access to medical care?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the incorporation of accessibility criteria into technology design processes by standards organizations. It is specific and has a clear intent, focusing on the actions of standards organizations regarding accessibility. However, it may benefit from a bit more specificity regarding which standards organizations or types of technology are being referred to, as this could help narrow down the scope of the answer. For example, specifying whether the question pertains to software, hardware, or web technologies could enhance clarity. Overall, the question is understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: \"How have standards organizations incorporated accessibility criteria into technology design processes?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The Equal Employment Opportunity Commission and the Department of Justice have clearly laid out how employers’ use of AI and other automated systems can result in discrimination against job applicants and employees with disabilities. The documents explain how employers’ use of software that relies on algorithmic decision-making may violate existing requirements', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the role of independent evaluation in preventing algorithmic discrimination in automated systems. It is specific and has a clear intent, focusing on a particular aspect of algorithmic fairness. The question can be understood and answered without needing additional context or external references, making it self-contained. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 2, 'structure': 3, 'relevance': 3, 'score': 2.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Technical standards', 'Accessibility', 'People with disabilities', 'User experience research']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['National AI Research Institutes', 'Safe and trustworthy AI algorithms', 'Cyber Physical Systems program', 'Secure and Trustworthy Cyberspace program', 'Formal Methods in the Field program']\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"How does allowing independent evaluation contribute to the prevention of algorithmic discrimination in automated systems?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and clear, asking about the impact of Section 508 regulations on technology design processes in the US, with a focus on accessibility for individuals with disabilities. It does not rely on external references and can be understood independently. The intent is clear, seeking information on how these regulations influence design practices. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question addresses the issue of avoiding inappropriate use of proxies in relation to target measures, which is a specific topic. However, it lacks clarity regarding what is meant by 'inappropriate use of proxies' and 'target measures'. The terms are somewhat vague and could refer to various contexts, such as statistical analysis, project management, or performance evaluation. To improve clarity and answerability, the question could specify the context in which proxies and target measures are being discussed, and provide examples of what constitutes inappropriate use. This would help in formulating a more precise and relevant response.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The public can be protected from algorithmic discrimination in a proactive and ongoing manner by reinforcing legal protections and extending them to ensure equity for underserved communities. These protections should be instituted throughout the design, development, and deployment process. Proactive equity assessments should be conducted in the design phase of the technology to assess equity in design.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the focus of the Secure and Trustworthy Cyberspace program?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How do Section 508 regulations affect US technology design for accessibility?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the prioritization of user experience and design decisions concerning privacy concerns. It is clear in its intent, seeking guidance on how to balance these two important aspects. However, the question could benefit from being more specific about the context or type of systems being referred to (e.g., mobile apps, websites, software platforms) to enhance clarity and answerability. Additionally, specifying what aspects of user experience and design decisions are of interest (e.g., user interface, data handling practices) would help in providing a more focused response.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: How should systems prioritize user experience and design decisions in relation to privacy concerns?\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Standards and guidance for the use of automated systems are being developed by federal government agencies to prevent bias. Non-profits and companies have also created best practices for audits and impact assessments to identify potential algorithmic discrimination and provide transparency to the public in mitigating such biases. However, more work is needed to protect the public from algorithmic discrimination and ensure that automated systems are designed and used in an equitable manner. Basic safeguards against abuse, bias, and discrimination should extend to both daily and digital lives.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is specific and seeks to understand the impact of the absence of federal law on private data collection and its implications for the American public's access to and control over personal data. It is clear in its intent and does not rely on external references, making it understandable and answerable based on the details provided. However, it could be improved by specifying the time frame or context (e.g., recent developments, specific incidents) to provide a clearer focus for the response. Overall, it meets the criteria for clarity and answerability.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How should automated systems ensure accessibility for people with disabilities?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the function of independent evaluation in relation to algorithmic discrimination in automated systems, maintaining the same constraints and depth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] evolution_filter failed, retrying with 1\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Algorithmic discrimination', 'Guardrails protecting the public', 'Mitigation of biases', 'Impact assessments']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Equity assessments', 'Accessibility designs', 'Disparity testing', 'Mitigation implementation', 'Algorithmic impact assessment']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the targeting of jobs with taxi companies towards Black people, but it lacks clarity and specificity. It is not clear what is meant by 'primarily targeted'—whether it refers to hiring practices, marketing strategies, or something else. Additionally, the question assumes a context that may not be universally understood, such as the socio-economic factors influencing employment in the taxi industry. To improve clarity and answerability, the question could specify the aspect of targeting being referred to (e.g., recruitment practices, outreach programs) and provide context or examples to clarify the intent.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the discriminatory practices of a healthcare algorithm against Black patients regarding access to medical care. It is specific in its focus on a particular demographic (Black patients) and the context of healthcare algorithms, making the intent clear. However, the question could benefit from additional context regarding which specific algorithm is being referenced, as there are many healthcare algorithms in use. Providing details about the algorithm or the study in which this discrimination was observed would enhance clarity and answerability. Overall, while the question is understandable, it could be improved by specifying the algorithm or study in question.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is complex and multifaceted, addressing the role of legal protections in ensuring equity for underserved communities in the context of automated systems. It specifies the focus on algorithmic discrimination and the use of representative data, which provides a clear intent. However, the question is quite broad and may require additional context to fully understand the specific legal protections being referred to, as well as the types of automated systems in question. To improve clarity and answerability, the question could be broken down into more specific components, such as asking about particular legal frameworks, examples of automated systems, or specific types of algorithmic discrimination. This would help in providing a more focused and direct response.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"How can legal protections ensure equity for underserved communities in the development and deployment of automated systems while proactively assessing for algorithmic discrimination and utilizing representative data?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question addresses the role of the ADA (Americans with Disabilities Act) in protecting individual rights within workplace and healthcare settings, while also considering disparities in healthcare access and biases in AI systems. It is specific in its focus on the ADA and the contexts of workplace and healthcare, making the intent clear. However, the question is somewhat complex and may require additional context regarding the specific aspects of the ADA being referenced, as well as the nature of the disparities and biases mentioned. To improve clarity and answerability, the question could be simplified or broken down into more focused sub-questions, such as: 'What provisions of the ADA specifically protect individuals in the workplace?' and 'How does the ADA address healthcare access disparities?' This would make it easier to provide a comprehensive answer.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: How has the lack of federal law addressing the expanding scale of private data collection affected the ability of American public to access and control their personal data?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question clearly asks about the contributions of specific organizations (the Access Board and the World Wide Web Consortium) to the incorporation of accessibility criteria in technology design processes. It is specific and independent, as it does not rely on external references or context to be understood. The intent is clear, seeking information about the roles these organizations play in promoting accessibility. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What steps are involved in the design stage equity assessments for accessibility designs?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What measures are being taken to protect the public from algorithmic discrimination and ensure equitable use and design of automated systems?\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about Section 508 regulations and their impact on technology design processes in the United States, focusing on accessibility. They share the same constraints and requirements, as well as similar depth and breadth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['American public', 'Government agencies', 'Surveillance capabilities', 'Private data collection', 'Data brokers']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 2, 'structure': 3, 'relevance': 3, 'score': 2.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Technical standards', 'Algorithmic discrimination', 'Anti-discrimination law', 'Proactive technical steps']\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How does the ADA protect individuals in workplace and healthcare, addressing healthcare access disparities and AI biases?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Cashier ads to women', 'Jobs with taxi companies to primarily Black people', 'Body scanners at airport checkpoints', 'Gender identity', 'Transgender travelers']\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"How do organizations like the Access Board and the World Wide Web Consortium contribute to incorporating accessibility criteria into technology design processes?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the role of government agencies in using and developing technologies for surveillance and their impact on the American public. While it is specific in its focus on government agencies and surveillance technologies, it lacks clarity in terms of what specific technologies or agencies are being referred to, and what aspects of the impact on the American public are of interest (e.g., privacy concerns, security benefits, legal implications). To improve clarity and answerability, the question could specify particular technologies (e.g., facial recognition, data collection methods) or agencies (e.g., NSA, FBI) and clarify the type of impact being considered (e.g., social, legal, ethical).', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How has the expanding scale of private data collection impacted the American public?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What proactive steps can be taken to ensure automated systems are free from algorithmic discrimination?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the effects of private data collection on the public, while the second question addresses the impact of the lack of federal laws on access to personal data. They differ in their specific focus and implications, leading to different depths and breadths of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How do body scanners at airport checkpoints impact transgender travelers and what steps are being taken to address this issue?\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Research and development', 'Acquisition review', 'Potential discrimination', 'Equity effects', 'Underserved communities']\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"The healthcare algorithm discriminated against Black patients by relying on the cost of each patient's past medical care to predict future medical needs. This process recommended early interventions for patients deemed most at risk, but it discriminated against Black patients who generally have less access to medical care and therefore have generated less cost.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the prioritization of user experience and design decisions concerning privacy concerns. It is clear in its intent, seeking guidance on how to balance these two important aspects. However, the question could benefit from being more specific about the context or type of systems being referred to (e.g., mobile apps, websites, software platforms) to enhance clarity and answerability. Additionally, specifying what aspects of user experience and design decisions are of interest (e.g., user interface, data handling practices) would help in providing a more focused response.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks how automated systems can ensure accessibility for people with disabilities. It is clear in its intent, specifying the focus on automated systems and the target audience of people with disabilities. The question is independent and does not rely on external references, making it understandable and answerable based on general knowledge of accessibility practices. However, it could be improved by specifying the types of automated systems (e.g., software, hardware, online services) or the specific disabilities being considered, which would provide a more focused context for the answer.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"How should automated systems ensure accessibility for people with disabilities?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Research and development', 'Acquisition review', 'Potential discrimination', 'Equity effects', 'Underserved communities']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How can research and development processes be made more inclusive and equitable for underserved communities?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question addresses the impact of discrimination in healthcare algorithms on the access of Black patients to healthcare services, specifically in the context of Title I of the Americans with Disabilities Act (ADA). It is clear in its intent to explore the relationship between algorithmic discrimination and healthcare access for a specific demographic group. However, the question could benefit from additional specificity regarding what aspects of discrimination or healthcare access are being considered, as well as how Title I of the ADA relates to these issues. To improve clarity and answerability, the question could specify the types of algorithms in question (e.g., predictive algorithms, risk assessment tools) and the particular dimensions of healthcare access being examined (e.g., availability of services, quality of care).', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the focus of the Secure and Trustworthy Cyberspace program, which is a specific program. It is clear and self-contained, as it does not rely on external references or additional context to understand what is being asked. The intent is also clear, seeking information about the program's objectives or areas of emphasis. Therefore, it meets the criteria for clarity and answerability.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: What is the focus of the Secure and Trustworthy Cyberspace program?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How can automated systems ensure accessibility for individuals with disabilities while also addressing algorithmic discrimination?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How can potential discrimination be identified and addressed in the introduction of new technology?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Algorithmic discrimination', 'Guardrails protecting the public', 'Mitigation of biases', 'Impact assessments']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the steps involved in the design stage equity assessments specifically for accessibility designs. It is clear in its intent, specifying the focus on 'design stage equity assessments' and 'accessibility designs'. However, it could benefit from a bit more specificity regarding what type of accessibility designs are being referred to (e.g., physical spaces, digital interfaces) or the context in which these assessments are being conducted (e.g., regulatory frameworks, best practices). Adding such details would enhance clarity and ensure that the answer is more targeted and relevant.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about measures to protect the public from algorithmic discrimination and ensure equitable use and design of automated systems. It is clear in its intent, specifying the focus on public protection and equitable design. However, the question is somewhat broad and may benefit from more specificity regarding the context (e.g., specific industries, types of automated systems, or geographical regions). To improve clarity and answerability, the question could specify which measures or frameworks are being referred to, or it could narrow down the scope to a particular area of concern (e.g., healthcare, finance, or law enforcement).', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What measures are being taken to protect the public from algorithmic discrimination and ensure equitable use and design of automated systems?\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: How does the discrimination in healthcare algorithms affect the healthcare access of Black patients, particularly in relation to Title I of the ADA?\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': \"The first question specifically addresses the incorporation of accessibility criteria by standards organizations into technology design processes, while the second question is more general and asks about organizations' help with accessibility in tech design. This difference in focus leads to a disparity in depth and breadth of inquiry.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The absence of federal laws has impacted American access to personal data by allowing government agencies and technology companies to collect and use personal data without adequate oversight or regulation. This lack of regulation has led to a situation where individuals often cannot access or control their personal data, and are unable to make informed decisions about its collection and use.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking about the impact of body scanners on transgender travelers and the measures being taken to address any issues that arise. It does not rely on external references and can be understood independently. The intent is also clear, as it seeks information on both the effects and the responses to those effects. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': \"Both questions inquire about the protections offered by the Americans with Disabilities Act (ADA) in the context of workplace and healthcare access, while also addressing similar themes of rights and disparities. They share the same depth and breadth of inquiry regarding the ADA's impact.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How are standards, guidance, audits, and impact assessments being used to mitigate algorithmic discrimination and bias in automated systems?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about proactive steps to prevent algorithmic discrimination in automated systems. It is clear in its intent, specifying the focus on proactive measures and the context of automated systems. The question is independent and can be understood without needing additional context or references. However, it could be improved by specifying the types of automated systems (e.g., AI, machine learning models) or the domains in which these systems operate (e.g., hiring, lending) to provide a more focused response. Overall, the question is specific and answerable as it stands.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What proactive measures should be implemented to safeguard the public from algorithmic discrimination and ensure fair utilization and creation of automated systems?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the impact of the expanding scale of private data collection on the American public. It is relatively clear in its intent, seeking to understand the effects of a specific phenomenon (private data collection) on a defined group (the American public). However, the question is somewhat broad and could benefit from more specificity regarding the type of impact being considered (e.g., privacy concerns, behavioral changes, legal implications). To improve clarity and answerability, the question could specify which aspects of the impact are of interest, such as psychological effects, changes in consumer behavior, or implications for civil liberties.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: How has the expanding scale of private data collection impacted the American public?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking about ways to enhance inclusivity and equity in research and development processes for underserved communities. It does not rely on external references and can be understood independently. The intent is evident, seeking actionable strategies or insights. However, to improve the question further, it could specify particular aspects of research and development (e.g., funding, participation, access to resources) or mention specific underserved communities to provide more context. This would help in generating more targeted and relevant responses.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"How can research and development processes be made more inclusive and equitable for underserved communities?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 2, 'structure': 3, 'relevance': 3, 'score': 2.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Technical standards', 'Algorithmic discrimination', 'Anti-discrimination law', 'Proactive technical steps']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is complex and multifaceted, addressing the role of legal protections in ensuring equity for underserved communities in the context of automated systems. It specifies the focus on algorithmic discrimination and the use of representative data, which provides a clear intent. However, the question is quite broad and may require additional context to fully understand the specific legal protections being referred to, as well as the types of automated systems in question. To improve clarity and answerability, the question could be broken down into more specific components, such as asking about particular legal frameworks, examples of automated systems, or specific types of algorithmic discrimination. This would help in providing a more focused and direct response.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and seeks information about a particular program related to cybersecurity and privacy enhancing technologies within the National AI Research Institutes. It is clear in its intent and does not rely on external references, making it understandable and answerable based on the details provided. However, it could be improved by specifying what kind of information is desired about the program (e.g., its goals, key projects, or funding). This would enhance clarity and allow for a more targeted response.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question addresses the dual challenge of ensuring accessibility for individuals with disabilities and tackling algorithmic discrimination within automated systems. It is specific in its focus on two important issues, making the intent clear. However, the question may be perceived as complex and multifaceted, which could lead to varying interpretations of what constitutes 'automated systems' and the specific types of accessibility and discrimination being referenced. To improve clarity and answerability, the question could be reframed to specify the types of automated systems (e.g., AI, software applications) and provide examples of accessibility measures and forms of algorithmic discrimination. This would help narrow down the scope and make it easier to provide a focused response.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The steps involved in the design stage equity assessments for accessibility designs include consultation, potentially qualitative analysis, accessibility designs and testing, disparity testing, documenting any remaining disparities, and detailing mitigation implementation and assessments. The algorithmic impact assessment should be made public whenever possible, with reporting provided in a clear and machine-readable manner using plain language for easier public accountability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What proactive steps can be taken to ensure automated systems are free from algorithmic discrimination?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How can R&D processes be enhanced to ensure inclusivity and equity for marginalized communities, considering factors like historical context, accessibility, and societal goals, and assessing impacts on various underserved groups such as BIPOC, LGBTQI+ individuals, persons with disabilities, and those affected by poverty or inequality?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"Standards organizations have developed guidelines to incorporate accessibility criteria into technology design processes. In the United States, the Access Board's Section 508 regulations are the most prevalent technical standards for federal information communication technology. Other standards are issued by the International Organization for Standardization and the World Wide Web Consortium Web Content Accessibility Guidelines, which is a globally recognized voluntary consensus standard for web content and other information and communications technology.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The proactive steps that can be taken to ensure automated systems are free from algorithmic discrimination include testing the system to ensure it is free from such discrimination before it can be sold or used. Protection against algorithmic discrimination should involve designing for equity and complying with existing anti-discrimination laws. These steps are crucial to prevent any form of discrimination in automated systems.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How can automated systems address accessibility and algorithmic discrimination for individuals with disabilities?\"\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"Which program focuses on cybersecurity and privacy enhancing technologies in automated systems within the National AI Research Institutes?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking for proactive measures to prevent algorithmic discrimination and promote fairness in automated systems. It does not rely on external references and can be understood independently. The intent is evident, as it seeks actionable recommendations. However, to enhance clarity, the question could specify the context or scope of the automated systems in question (e.g., specific industries, types of algorithms) to guide the response more effectively. Overall, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated systems', 'Technical standards', 'Algorithmic discrimination', 'Independent evaluation', 'Public sector uses']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the impact of the expanding scale of private data collection on the American public. It is relatively clear in its intent, seeking to understand the effects of a specific phenomenon (private data collection) on a defined group (the American public). However, the question is somewhat broad and could benefit from more specificity regarding the type of impacts being considered (e.g., privacy concerns, behavioral changes, legal implications). To improve clarity and answerability, the question could specify which aspects of the impact are of interest, such as psychological effects, changes in consumer behavior, or implications for civil liberties.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the use of standards, guidance, audits, and impact assessments in mitigating algorithmic discrimination and bias in automated systems. It is clear in its intent and specifies the areas of interest (standards, guidance, audits, impact assessments) and their purpose (mitigating discrimination and bias). However, the question is somewhat broad and may benefit from narrowing down the focus to specific types of automated systems or particular standards and guidelines. To improve clarity and answerability, the question could specify which automated systems are being referred to (e.g., AI in hiring, facial recognition) or ask for examples of specific standards or audits that have been effective in this context.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"Body scanners at airport checkpoints impact transgender travelers by requiring the operator to select a 'male' or 'female' scanning setting based on the passenger's sex, which may not align with the passenger's gender identity. This can lead to transgender travelers being flagged for extra screening, resulting in degrading experiences. To address this issue, TSA has announced plans to implement a gender-neutral algorithm while enhancing the security effectiveness capabilities of the existing technology.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How can we prevent algorithmic discrimination and promote fairness in automated systems?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions address the intersection of healthcare access and the Americans with Disabilities Act (ADA) in relation to Black patients, but they focus on different aspects: one emphasizes the significance of Title I while the other discusses algorithm discrimination. This leads to differences in depth and breadth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What is the importance of independent evaluations for algorithmic discrimination in public sector uses of automated systems?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses solely on ensuring accessibility for people with disabilities, while the second question expands the inquiry to include algorithmic discrimination, adding complexity and depth. Therefore, they do not have the same breadth and depth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about identifying and addressing potential discrimination in the context of new technology. It is clear in its intent, focusing on a specific issue (discrimination) related to a specific context (introduction of new technology). However, the question could benefit from being more specific about the type of technology or the context in which it is being introduced (e.g., healthcare, AI, etc.), as this could lead to more targeted and relevant responses. Additionally, specifying the stakeholders involved (e.g., developers, users, policymakers) could enhance clarity. Overall, while the question is understandable, adding these details would improve its answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about proactive steps to prevent algorithmic discrimination in automated systems. It is clear in its intent, specifying the focus on proactive measures and the context of automated systems. The question is independent and can be understood without needing additional context or references. However, it could be improved by specifying the types of automated systems (e.g., AI, machine learning models) or the domains of application (e.g., hiring, lending) to provide a more focused response. Overall, the question is specific and answerable as it stands.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What proactive steps can be taken to ensure automated systems are free from algorithmic discrimination?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question inquires about the focus of a specific program, while the second question asks about a different program related to cybersecurity and privacy in AI systems. They do not share the same constraints or depth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What measures can be implemented to ensure automated systems are devoid of algorithmic discrimination?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 3, 'depth': 2, 'structure': 3, 'relevance': 3, 'score': 2.75}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Accessibility criteria', 'Technology design processes', 'Section 508 regulations', 'Web Content Accessibility Guidelines', 'Managing Bias']\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'the Secure and Trustworthy Cyberspace program supports research on cybersecurity and privacy enhancing technologies in automated systems', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is quite complex and multifaceted, addressing how R&D processes can be improved for inclusivity and equity for marginalized communities. While it specifies various factors to consider (historical context, accessibility, societal goals) and identifies specific underserved groups (BIPOC, LGBTQI+, persons with disabilities, those affected by poverty or inequality), the breadth of the question may lead to ambiguity in what specific aspect of R&D processes is being targeted for enhancement. Additionally, the question is lengthy and may overwhelm the respondent. To improve clarity and answerability, the question could be broken down into more focused sub-questions, such as: 'What specific strategies can be implemented in R&D to enhance inclusivity for BIPOC communities?' or 'How can accessibility be improved in R&D processes for individuals with disabilities?'. This would allow for more direct and manageable responses.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"How can R&D processes be enhanced to ensure inclusivity and equity for marginalized communities, considering factors like historical context, accessibility, and societal goals, and assessing impacts on various underserved groups such as BIPOC, LGBTQI+ individuals, persons with disabilities, and those affected by poverty or inequality?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What are the Section 508 regulations and how do they impact technology design processes in the United States?\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions address the issue of algorithmic discrimination and the need for fairness in automated systems. They share similar constraints and requirements, focusing on protective measures and promoting equity, thus having the same depth and breadth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Healthcare algorithm discrimination impacts Black patient access by perpetuating disparities in healthcare access. For example, a widely used healthcare algorithm that relied on past medical costs to predict future needs discriminated against Black patients who generally have less access to medical care. This discrimination can violate ADA Title I, which prohibits discrimination against individuals with disabilities, including those related to healthcare access.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Standards and guidance for the use of automated systems are being developed by federal government agencies to prevent bias. Non-profits and companies have also developed best practices for audits and impact assessments to identify potential algorithmic discrimination and provide transparency to the public in mitigating such biases. However, more work is needed to protect the public from algorithmic discrimination and ensure the equitable use and design of automated systems.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the significance of independent evaluations concerning algorithmic discrimination in the context of public sector automated systems. It is clear in its intent, specifying the focus on independent evaluations and their importance in relation to algorithmic discrimination. The question is self-contained and does not rely on external references, making it understandable and answerable based on the details provided. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What is the importance of independent evaluations for algorithmic discrimination in public sector uses of automated systems?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Potential discrimination in the introduction of new technology can be identified and addressed by reviewing potential input data, associated historical context, accessibility for people with disabilities, and societal goals. It is important to assess the impact on underserved communities such as Black, Latino, Indigenous and Native American persons, Asian Americans and Pacific Islanders, members of religious minorities, women, girls, non-binary people, and LGBTQ+ individuals to ensure equity and prevent discrimination.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Automated systems can address accessibility for individuals with disabilities by ensuring that they are designed, developed, and deployed in ways that are accessible to people with a wide variety of disabilities. This includes adherence to relevant accessibility standards and conducting user experience research to identify and address any accessibility barriers. In addition, automated systems should demonstrate that they protect against algorithmic discrimination by allowing independent evaluation of potential discrimination caused by the systems.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['National Disabled Law Students Association', 'Remote proctoring AI systems', 'Patients with high needs for healthcare', 'Black patients', 'Healthcare clinical algorithms']\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"Why is independent evaluation crucial for identifying algorithmic discrimination in public sector use of automated systems?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for an explanation of Section 508 regulations and their impact on technology design processes in the United States. It is clear in its intent, specifying both the regulations and the context of technology design. The question is self-contained and does not rely on external references, making it understandable and answerable based on the details provided. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: What are the Section 508 regulations and how do they impact technology design processes in the United States?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How do healthcare clinical algorithms impact the treatment of patients with different racial backgrounds?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about measures to prevent algorithmic discrimination in automated systems. It is specific and has a clear intent, focusing on a relevant and important topic in the field of technology and ethics. The question is independent, as it does not rely on external references or context to be understood. However, it could be improved by specifying the type of automated systems in question (e.g., hiring algorithms, credit scoring systems) or the context in which these measures are to be applied. This would help narrow down the scope and provide more targeted answers.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How to prevent algorithmic discrimination in automated systems?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question is quite complex and multifaceted, asking how R&D processes can be improved for inclusivity and equity for marginalized communities while considering various factors and impacts on specific underserved groups. While it is clear in its intent to explore enhancements in R&D processes, the breadth of the question may lead to ambiguity in terms of what specific aspects of R&D are being referred to (e.g., funding, methodology, stakeholder engagement). Additionally, the question assumes a level of familiarity with the concepts of inclusivity and equity in R&D that may not be universally understood. To improve clarity and answerability, the question could be broken down into more specific sub-questions or focus on particular aspects of R&D processes, such as 'What specific strategies can be implemented in R&D to enhance inclusivity for BIPOC communities?' This would allow for more targeted and actionable responses.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the importance of independent evaluation in detecting algorithmic discrimination within the context of public sector automated systems. It is specific and has a clear intent, focusing on the relationship between independent evaluation and algorithmic discrimination. The question can be understood and answered without needing additional context or references, making it self-contained. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Large disparities by race', 'Risk scores', 'Guide students', 'Math and science subjects', 'Risk assessment tool']\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"Why is independent evaluation important for detecting algorithmic discrimination in public sector automated systems?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions address the issue of algorithmic discrimination in automated systems and seek solutions to prevent it. They have similar constraints and requirements, as well as comparable depth and breadth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How do risk scores impact the guidance of Black students towards certain majors?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the impact of healthcare clinical algorithms on the treatment of patients with different racial backgrounds. It is clear in its intent to explore the relationship between clinical algorithms and racial disparities in treatment. However, the question could benefit from being more specific about which algorithms are being referred to, as well as the types of treatments or outcomes being considered. Additionally, it could clarify whether it seeks a general overview or specific examples. To improve clarity and answerability, the question could specify particular algorithms or treatment areas (e.g., chronic disease management, mental health) and the nature of the impact being investigated (e.g., effectiveness, accessibility).', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How do healthcare clinical algorithms impact the treatment of patients with different racial backgrounds?\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Algorithmic discrimination', 'Automated systems', 'Unjustified different treatment', 'Protected classifications', 'Legal protections']\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How do racial biases in healthcare algorithms affect the treatment of patients from diverse backgrounds?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the significance of independent evaluations in relation to algorithmic discrimination within public sector automated systems, sharing the same constraints and depth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"What are some examples of protected classifications that should not be used to discriminate against individuals in automated systems?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the impact of Section 508 regulations on technology design processes in the US and inquires about other relevant standards. It is clear in its intent, specifying the guidelines (Section 508) and the context (technology design processes in the US). However, the question could be improved by providing a bit more detail on what aspects of technology design processes are being considered (e.g., accessibility, usability) and what types of standards are being referred to (e.g., industry standards, legal standards). This would help to narrow down the scope and make the question more focused. Overall, it is understandable and answerable based on the details provided, but slight refinements could enhance clarity.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Algorithmic Discrimination Protections', 'Automated systems', 'Inequitable outcomes', 'Facial recognition technology', 'Discriminatory decisions']\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"How do guidelines like Section 508 regulations impact technology design processes in the US, and what other standards are relevant in this context?\"\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How can automated systems contribute to discriminatory decisions?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the impact of risk scores on the guidance of Black students towards specific majors. It is relatively clear in its intent, focusing on the relationship between risk scores and academic guidance for a particular demographic. However, the term 'risk scores' could be ambiguous without further context, as it may refer to various types of assessments or metrics. To improve clarity and answerability, the question could specify what type of risk scores are being referred to (e.g., academic risk, socioeconomic risk) and how these scores are used in the guidance process. Additionally, providing context on the educational setting or framework in which this guidance occurs would enhance understanding.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: How do risk scores impact the guidance of Black students towards certain majors?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question addresses the impact of racial biases in healthcare algorithms on the treatment of patients from diverse backgrounds. It is specific and clear in its intent, seeking to understand the relationship between algorithmic bias and patient treatment outcomes. The question is independent, as it does not rely on external references or additional context to be understood. However, it could be improved by specifying the types of healthcare algorithms being referred to or providing examples of how these biases manifest in treatment. This would enhance clarity and allow for a more focused response.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking for examples of protected classifications that should not be used for discrimination in automated systems. It does not rely on external references and can be understood independently. The intent is straightforward, seeking specific examples related to discrimination and protected classifications. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"What are some examples of protected classifications that should not be used to discriminate against individuals in automated systems?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the impact of Section 508 regulations on technology design processes in the United States, but the second question introduces additional standards, which expands the breadth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How do healthcare algorithms impact treatment for diverse patients due to racial biases?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What are some examples of protected classifications that should not be used to discriminate against individuals in automated systems, potentially leading to unjust treatment or impacts?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the role of automated systems in contributing to discriminatory decisions. It is clear in its intent and specifies the topic of interest (automated systems and discriminatory decisions), making it understandable. However, the question could benefit from being more specific about the type of automated systems (e.g., algorithms, AI models) and the context in which these decisions are made (e.g., hiring, law enforcement, lending). Adding these details would enhance clarity and allow for a more focused and relevant response.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How can automated systems contribute to discriminatory decisions?\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions explore the impact of healthcare algorithms on patient treatment with a focus on racial backgrounds and biases, sharing the same depth and breadth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Section 508 regulations impact US tech design processes by setting technical standards for federal information communication technology. Other important standards include those from the International Organization for Standardization and the World Wide Web Consortium Web Content Accessibility Guidelines.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How do automated systems potentially contribute to discriminatory outcomes based on unjustified differential treatment according to protected classifications?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the impact of risk scores on the guidance of Black students towards specific majors. It is relatively clear in its intent, focusing on the relationship between risk scores and academic guidance for a particular demographic. However, the term 'risk scores' could be ambiguous without further context, as it may refer to various types of assessments or metrics. To improve clarity and answerability, the question could specify what type of risk scores are being referred to (e.g., academic risk, socioeconomic risk) and how these scores are used in the guidance process. Additionally, providing context on the educational system or framework in which this guidance occurs could enhance understanding.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 3 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking for examples of protected classifications that should not be used for discrimination in automated systems. It conveys its intent without ambiguity, allowing for a direct response. The phrasing indicates a focus on ethical considerations in automated systems, which is a well-defined area of inquiry. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Americans with Disabilities Act', 'Title I', 'Disparity assessments', 'Healthcare algorithm', \"Black patients' healthcare access\"]\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What are some protected classifications that should not be used in automated systems to avoid unjust treatment?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Automated system', 'Algorithmic discrimination', 'Demographic information', 'Riskier systems', 'Disparity mitigation']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How does the Americans with Disabilities Act protect the rights of individuals in the workplace and in healthcare access?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How can demographic information be used to assess algorithmic discrimination in automated systems?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question addresses the potential contribution of automated systems to discriminatory outcomes, specifically focusing on unjustified differential treatment based on protected classifications. It is clear in its intent to explore the relationship between automated systems and discrimination, making it understandable. However, the phrasing is somewhat complex and may benefit from simplification for broader clarity. To improve answerability, the question could specify what types of automated systems are being referred to (e.g., algorithms in hiring, credit scoring) and provide examples of protected classifications (e.g., race, gender) to ground the inquiry. This would help in making the question more accessible and focused.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about protected classifications in the context of automated systems and their implications for discrimination and unjust treatment. They share the same constraints and requirements, as well as similar depth and breadth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 3 times\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How can automated systems lead to discrimination based on unjustified differences in protected categories?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Protection Bureau', 'Prudential regulators', 'Automated Valuation Models', 'Equal Employment Opportunity Commission', 'Department of Justice']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question clearly asks about the protections offered by the Americans with Disabilities Act (ADA) regarding workplace rights and healthcare access for individuals with disabilities. It specifies the two contexts (workplace and healthcare) and seeks information on the rights protected under the ADA, making the intent clear and the question self-contained. Therefore, it can be understood and answered without needing additional context or references. Overall, the question meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How does the Americans with Disabilities Act protect the rights of individuals in the workplace and in healthcare access?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How has the Equal Employment Opportunity Commission addressed discrimination related to employers' use of AI and automated systems?\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the role of automated systems in discrimination, focusing on similar themes of decision-making and unjust differences. They share the same depth and breadth of inquiry regarding the implications of automated systems.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 3 times\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How does the ADA safeguard the rights of individuals in workplace and healthcare, considering disparities in healthcare access and potential biases in AI systems?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks how demographic information can be utilized to evaluate algorithmic discrimination in automated systems. It is specific and has a clear intent, focusing on the relationship between demographic data and the assessment of discrimination in algorithms. The question is independent and can be understood without needing additional context or references. However, it could be improved by specifying what types of automated systems are being referred to (e.g., hiring algorithms, credit scoring systems) or what aspects of algorithmic discrimination are of interest (e.g., fairness, bias). This would provide a more focused framework for the answer.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"How can demographic information be used to assess algorithmic discrimination in automated systems?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Legal protections', 'Equity for underserved communities', 'Algorithmic discrimination', 'Proactive equity assessments', 'Automated systems']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How can the public be protected from algorithmic discrimination in a proactive and ongoing manner?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How can demographic data aid in evaluating algorithmic bias in automated systems to ensure equitable outcomes and mitigate disparities?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the actions taken by the Equal Employment Opportunity Commission (EEOC) regarding discrimination linked to the use of AI and automated systems by employers. It is specific and clear in its intent, focusing on a particular agency (EEOC) and a specific issue (discrimination related to AI). However, it may require some background knowledge about the EEOC's policies and recent developments in AI regulations to fully answer. To enhance clarity and answerability, the question could specify a time frame (e.g., recent actions, historical context) or particular aspects of AI use that are of interest (e.g., hiring practices, performance evaluations).\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How has the Equal Employment Opportunity Commission addressed discrimination related to employers' use of AI and automated systems?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question addresses the role of the ADA (Americans with Disabilities Act) in protecting individual rights within workplace and healthcare settings, while also considering disparities in healthcare access and biases in AI systems. It is specific in its focus on the ADA and the contexts of workplace and healthcare, making the intent clear. However, the question is somewhat complex and may require additional context regarding the specific aspects of the ADA being referenced, as well as the nature of the disparities and biases mentioned. To improve clarity and answerability, the question could be simplified or broken down into more focused sub-questions, such as: 'What provisions of the ADA specifically protect individuals in the workplace?' and 'How does the ADA address healthcare access disparities?' This would make it easier to provide a comprehensive answer.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How has the EEOC addressed AI-related discrimination in employer practices and what proactive measures are recommended for system developers?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How does the ADA protect individuals in workplace and healthcare, addressing healthcare access disparities and AI biases?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question addresses the issue of protecting the public from algorithmic discrimination, focusing on proactive and ongoing measures. It is clear in its intent and does not rely on external references, making it understandable and answerable. However, it could benefit from being more specific about the context or domain (e.g., specific industries, types of algorithms, or populations affected) to guide the response more effectively. For improved clarity, the question could specify what kind of measures are being considered (e.g., policy changes, technological solutions, public awareness campaigns).', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How can the public be protected from algorithmic discrimination in a proactive and ongoing manner?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking about the role of demographic data in evaluating algorithmic bias in automated systems. It conveys a clear intent to understand how demographic data can contribute to ensuring equitable outcomes and mitigating disparities. The question is independent and does not rely on external references or additional context, making it understandable and answerable based on the details provided. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How does demographic data help assess algorithmic bias in automated systems for fair outcomes and reducing disparities?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the protections offered by the Americans with Disabilities Act (ADA) in the context of workplace and healthcare access, but the second question introduces additional elements such as healthcare access disparities and AI biases, which expands the breadth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How can equity be ensured in the design and deployment of automated systems to prevent algorithmic discrimination in a proactive and ongoing manner?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking about the actions taken by the EEOC regarding AI-related discrimination in employer practices and the recommended proactive measures for system developers. It does not rely on external references and can be understood independently. The intent is clear, seeking both historical actions and recommendations. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How has the EEOC addressed AI-related discrimination and what proactive measures are recommended for developers?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions focus on the role of demographic information in evaluating algorithmic discrimination or bias in automated systems. They share similar constraints and requirements, as well as comparable depth and breadth of inquiry regarding fairness and disparities.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 3 times\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The Americans with Disabilities Act (ADA) protects individuals in the workplace by providing technical assistance to employers on how to comply with the ADA and to job applicants and employees who believe their rights may have been violated. In healthcare, the ADA addresses disparities by ensuring equal access to healthcare for individuals with disabilities. It also addresses AI biases by highlighting concerns that individuals with disabilities may be flagged as suspicious by remote proctoring AI systems due to their specific access needs, such as longer breaks or the use of assistive technologies like screen readers or dictation software.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': \"Both questions inquire about the EEOC's actions regarding AI-related discrimination, but the second question introduces an additional aspect by asking for proactive measures for developers, which adds depth to the inquiry.\", 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Algorithmic discrimination', 'Automated systems', 'Unjustified different treatment', 'Protected classifications', 'Legal protections']\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question addresses the important topic of ensuring equity in the design and deployment of automated systems to prevent algorithmic discrimination. It is specific in its focus on equity, automated systems, and the proactive and ongoing nature of the measures to be taken. However, the question could benefit from clearer intent regarding what specific aspects of equity or algorithmic discrimination are being referred to, as well as what kind of measures or frameworks might be considered. To improve clarity and answerability, the question could specify whether it seeks theoretical frameworks, practical strategies, or examples of existing systems that have successfully implemented such measures.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How is unjustified different treatment defined in the context of algorithmic discrimination?\"\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How can equity be maintained in automated systems to prevent discrimination?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions address the issue of preventing discrimination in automated systems, focusing on proactive measures and equity. They share similar constraints and depth of inquiry regarding algorithmic fairness.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 4 times\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The Equal Employment Opportunity Commission and the Department of Justice have clearly laid out how employers’ use of AI and other automated systems can result in discrimination against job applicants and employees with disabilities. They explain how employers’ use of software that relies on algorithmic decision-making may violate existing requirements. Proactive measures recommended for developers include ensuring that AI systems do not perpetuate biases, conducting regular audits to detect and address discriminatory outcomes, and providing transparency in the decision-making process.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks for a definition of 'unjustified different treatment' specifically in the context of algorithmic discrimination. It is clear in its intent and specifies the topic of interest, making it understandable and answerable without needing additional context. However, it could be improved by providing a brief explanation of what is meant by 'algorithmic discrimination' to ensure that all readers have a common understanding of the term. This would enhance clarity and make the question more accessible to those who may not be familiar with the concept.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"How is unjustified different treatment defined in the context of algorithmic discrimination?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 2, 'score': 1.25}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 4 times\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How is unjustified different treatment, based on protected classifications, defined in the context of algorithmic discrimination impacting Black Americans' health outcomes?\"\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 2, 'score': 1.25}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 4 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Research and development', 'Acquisition review', 'Potential discrimination', 'Equity effects', 'Underserved communities']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: \"How can the assessment of technology consider potential discrimination and effects on equity for underserved communities?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question seeks to define 'unjustified different treatment' based on protected classifications in the context of algorithmic discrimination affecting Black Americans' health outcomes. It is specific in its focus on a particular issue (algorithmic discrimination) and demographic (Black Americans), which contributes to its clarity. However, the term 'unjustified different treatment' may require further clarification or context for those unfamiliar with legal or ethical frameworks surrounding discrimination. To enhance clarity and answerability, the question could specify the legal or theoretical framework being referenced (e.g., civil rights law, ethical guidelines) or provide examples of what constitutes 'unjustified different treatment'.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How is algorithmic discrimination affecting Black Americans' health outcomes defined?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question addresses the assessment of technology in relation to potential discrimination and its effects on equity for underserved communities. It is clear in its intent to explore how technology assessments can incorporate considerations of discrimination and equity. However, the question could benefit from being more specific about the type of technology being assessed or the context in which this assessment is taking place (e.g., policy-making, product development, etc.). Additionally, clarifying what is meant by 'assessment' (e.g., evaluation criteria, methodologies) could enhance understanding. Overall, while the question is relatively clear, adding specificity would improve its clarity and answerability.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: \"How can the assessment of technology consider potential discrimination and effects on equity for underserved communities?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the definition of unjustified different treatment in algorithmic discrimination, while the second question specifically addresses the impact of algorithmic discrimination on the health outcomes of Black Americans. They differ in their specific focus and depth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"How can the evaluation of technology address potential discrimination and equity impacts on marginalized communities, including Black, Latino, Indigenous, Asian American, Pacific Islander, religious minorities, women, LGBTQI+ individuals, older adults, persons with disabilities, rural residents, and those affected by poverty or inequality?\"\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question seeks to explore how technology evaluation can address discrimination and equity impacts on various marginalized communities. It is specific in its intent, clearly identifying the groups of interest and the overarching theme of discrimination and equity. However, the question is quite broad and complex, which may make it challenging to answer comprehensively without additional context or clarification on what aspects of technology evaluation are being considered (e.g., metrics, methodologies, case studies). To improve clarity and answerability, the question could be reframed to focus on a specific technology or evaluation method, or it could ask for examples of successful evaluations that have addressed these issues. This would help narrow down the scope and make it easier to provide a focused response.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: \"How can the evaluation of technology address potential discrimination and equity impacts on marginalized communities, including Black, Latino, Indigenous, Asian American, Pacific Islander, religious minorities, women, LGBTQI+ individuals, older adults, persons with disabilities, rural residents, and those affected by poverty or inequality?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is quite broad and complex, asking how technology evaluation can address discrimination and equity impacts on a wide range of marginalized communities. While it identifies specific groups affected by discrimination, the question lacks clarity in terms of what aspects of technology evaluation are being referred to (e.g., metrics, methodologies, frameworks) and how these evaluations should be conducted. To improve clarity and answerability, the question could specify the type of technology being evaluated, the context in which the evaluation is taking place, and the specific equity impacts of interest. Additionally, breaking down the question into more focused sub-questions could help in providing a more structured and detailed response.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 5 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Principles into practice', 'Real-life examples', 'Combat discrimination', 'Mortgage lending', 'Nationwide initiative']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How is the federal government working to combat discrimination in mortgage lending?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the actions taken by the federal government to address discrimination in mortgage lending. It is specific and has a clear intent, focusing on a particular issue (discrimination in mortgage lending) and the entity involved (federal government). The question can be understood and answered without needing additional context or external references, making it self-contained. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How is the federal government working to combat discrimination in mortgage lending?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: \"What nationwide initiative has the Department of Justice launched to address discrimination in mortgage lending, and how are federal agencies collaborating to combat redlining?\"\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is specific and clear, asking about a nationwide initiative by the Department of Justice related to discrimination in mortgage lending and the collaboration of federal agencies to combat redlining. It does not rely on external references and can be understood independently. The intent is clear, seeking information about a specific initiative and the collaborative efforts involved. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What initiative is DOJ leading to tackle mortgage lending discrimination and how are agencies working together against redlining?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': \"The first question focuses on the federal government's overall efforts to combat discrimination in mortgage lending, while the second question specifically addresses a DOJ initiative and collaboration among agencies against redlining. This indicates a difference in scope and specificity, leading to different depths of inquiry.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The Department of Justice (DOJ) is leading a nationwide initiative to combat redlining in mortgage lending. This initiative involves reviewing how lenders who may be avoiding serving communities of color are conducting targeted marketing and advertising. The initiative will draw upon strong partnerships across federal agencies, including the Consumer Financial Protection Bureau.', 'verdict': 1}\n"
     ]
    }
   ],
   "source": [
    "testset = generator.generate_with_langchain_docs(test_split_documents, test_size=20, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How are standards, guidance, audits, and impac...</td>\n",
       "      <td>[launched, preventing harm to the public. Fede...</td>\n",
       "      <td>Standards and guidance for the use of automate...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How did the healthcare algorithm discriminate ...</td>\n",
       "      <td>[under Title I of the Americans with Disabilit...</td>\n",
       "      <td>The healthcare algorithm discriminated against...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What proactive steps can be taken to ensure au...</td>\n",
       "      <td>[WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\...</td>\n",
       "      <td>The proactive steps that can be taken to ensur...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What steps are involved in the design stage eq...</td>\n",
       "      <td>[consultation, design stage equity assessments...</td>\n",
       "      <td>The steps involved in the design stage equity ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How has the Department of Justice addressed th...</td>\n",
       "      <td>[Protection Bureau and prudential regulators. ...</td>\n",
       "      <td>The Equal Employment Opportunity Commission an...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/Blueprint-for-an-AI-Bill-of-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How are standards, guidance, audits, and impac...   \n",
       "1  How did the healthcare algorithm discriminate ...   \n",
       "2  What proactive steps can be taken to ensure au...   \n",
       "3  What steps are involved in the design stage eq...   \n",
       "4  How has the Department of Justice addressed th...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [launched, preventing harm to the public. Fede...   \n",
       "1  [under Title I of the Americans with Disabilit...   \n",
       "2  [WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\\...   \n",
       "3  [consultation, design stage equity assessments...   \n",
       "4  [Protection Bureau and prudential regulators. ...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  Standards and guidance for the use of automate...         simple   \n",
       "1  The healthcare algorithm discriminated against...         simple   \n",
       "2  The proactive steps that can be taken to ensur...         simple   \n",
       "3  The steps involved in the design stage equity ...         simple   \n",
       "4  The Equal Employment Opportunity Commission an...         simple   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "1  [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "2  [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "3  [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  \n",
       "4  [{'source': 'data/Blueprint-for-an-AI-Bill-of-...          True  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def generate_answers(chain, testset):\n",
    "  answers = []\n",
    "  contexts = []\n",
    "  questions = testset.to_pandas()[\"question\"].values.tolist()\n",
    "  ground_truths = testset.to_pandas()[\"ground_truth\"].values.tolist()\n",
    "\n",
    "  for question in tqdm.tqdm(questions):\n",
    "    # answer = chain.invoke({\"question\" : question})\n",
    "    # answers.append(answer[\"response\"])\n",
    "    # contexts.append([context.page_content for context in answer[\"context\"]])\n",
    "    answer = chain.invoke({\"question\": question})\n",
    "    # Extract the content from AIMessage if necessary\n",
    "    response = answer[\"response\"].content if hasattr(answer[\"response\"], 'content') else answer[\"response\"]\n",
    "    answers.append(response)\n",
    "    contexts.append([context.page_content for context in answer[\"context\"]])\n",
    "\n",
    "  return Dataset.from_dict({\n",
    "      \"question\" : questions,\n",
    "      \"answer\" : answers,\n",
    "      \"contexts\" : contexts,\n",
    "      \"ground_truth\" : ground_truths\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:53<00:00,  2.69s/it]\n"
     ]
    }
   ],
   "source": [
    "base_dataset = generate_answers(retrieval_augmented_qa_chain, testset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:53<00:00,  2.67s/it]\n"
     ]
    }
   ],
   "source": [
    "finetune_dataset = generate_answers(finetune_rag_chain, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1059f8122cdb4b3c9bef047982de2068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    base_dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.9417, 'context_recall': 0.9000}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How are standards, guidance, audits, and impac...</td>\n",
       "      <td>[launched, preventing harm to the public. Fede...</td>\n",
       "      <td>Federal government agencies have been developi...</td>\n",
       "      <td>Standards and guidance for the use of automate...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How did the healthcare algorithm discriminate ...</td>\n",
       "      <td>[under Title I of the Americans with Disabilit...</td>\n",
       "      <td>The healthcare algorithm discriminated against...</td>\n",
       "      <td>The healthcare algorithm discriminated against...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What proactive steps can be taken to ensure au...</td>\n",
       "      <td>[and deployers of automated systems should tak...</td>\n",
       "      <td>Proactive steps that can be taken to ensure au...</td>\n",
       "      <td>The proactive steps that can be taken to ensur...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What steps are involved in the design stage eq...</td>\n",
       "      <td>[consultation, design stage equity assessments...</td>\n",
       "      <td>The steps involved in the design stage equity ...</td>\n",
       "      <td>The steps involved in the design stage equity ...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How has the Department of Justice addressed th...</td>\n",
       "      <td>[the severity of certain diseases in Black Ame...</td>\n",
       "      <td>The Department of Justice has clearly laid out...</td>\n",
       "      <td>The Equal Employment Opportunity Commission an...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How are standards, guidance, audits, and impac...   \n",
       "1  How did the healthcare algorithm discriminate ...   \n",
       "2  What proactive steps can be taken to ensure au...   \n",
       "3  What steps are involved in the design stage eq...   \n",
       "4  How has the Department of Justice addressed th...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [launched, preventing harm to the public. Fede...   \n",
       "1  [under Title I of the Americans with Disabilit...   \n",
       "2  [and deployers of automated systems should tak...   \n",
       "3  [consultation, design stage equity assessments...   \n",
       "4  [the severity of certain diseases in Black Ame...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Federal government agencies have been developi...   \n",
       "1  The healthcare algorithm discriminated against...   \n",
       "2  Proactive steps that can be taken to ensure au...   \n",
       "3  The steps involved in the design stage equity ...   \n",
       "4  The Department of Justice has clearly laid out...   \n",
       "\n",
       "                                        ground_truth  context_precision  \\\n",
       "0  Standards and guidance for the use of automate...           1.000000   \n",
       "1  The healthcare algorithm discriminated against...           1.000000   \n",
       "2  The proactive steps that can be taken to ensur...           1.000000   \n",
       "3  The steps involved in the design stage equity ...           0.833333   \n",
       "4  The Equal Employment Opportunity Commission an...           1.000000   \n",
       "\n",
       "   context_recall  \n",
       "0        1.000000  \n",
       "1        1.000000  \n",
       "2        0.666667  \n",
       "3        1.000000  \n",
       "4        1.000000  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6bc49a40bc4c2383bf7616a9eff3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fine_tuned_result = evaluate(\n",
    "    finetune_dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.9347, 'context_recall': 0.9500}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How are standards, guidance, audits, and impac...</td>\n",
       "      <td>[launched, preventing harm to the public. Fede...</td>\n",
       "      <td>Federal government agencies have been developi...</td>\n",
       "      <td>Standards and guidance for the use of automate...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How did the healthcare algorithm discriminate ...</td>\n",
       "      <td>[under Title I of the Americans with Disabilit...</td>\n",
       "      <td>The healthcare algorithm discriminated against...</td>\n",
       "      <td>The healthcare algorithm discriminated against...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What proactive steps can be taken to ensure au...</td>\n",
       "      <td>[and deployers of automated systems should tak...</td>\n",
       "      <td>Proactive steps that can be taken to ensure au...</td>\n",
       "      <td>The proactive steps that can be taken to ensur...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What steps are involved in the design stage eq...</td>\n",
       "      <td>[consultation, design stage equity assessments...</td>\n",
       "      <td>The design stage equity assessments for access...</td>\n",
       "      <td>The steps involved in the design stage equity ...</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How has the Department of Justice addressed th...</td>\n",
       "      <td>[the severity of certain diseases in Black Ame...</td>\n",
       "      <td>The Department of Justice has clearly laid out...</td>\n",
       "      <td>The Equal Employment Opportunity Commission an...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How are standards, guidance, audits, and impac...   \n",
       "1  How did the healthcare algorithm discriminate ...   \n",
       "2  What proactive steps can be taken to ensure au...   \n",
       "3  What steps are involved in the design stage eq...   \n",
       "4  How has the Department of Justice addressed th...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [launched, preventing harm to the public. Fede...   \n",
       "1  [under Title I of the Americans with Disabilit...   \n",
       "2  [and deployers of automated systems should tak...   \n",
       "3  [consultation, design stage equity assessments...   \n",
       "4  [the severity of certain diseases in Black Ame...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Federal government agencies have been developi...   \n",
       "1  The healthcare algorithm discriminated against...   \n",
       "2  Proactive steps that can be taken to ensure au...   \n",
       "3  The design stage equity assessments for access...   \n",
       "4  The Department of Justice has clearly laid out...   \n",
       "\n",
       "                                        ground_truth  context_precision  \\\n",
       "0  Standards and guidance for the use of automate...           1.000000   \n",
       "1  The healthcare algorithm discriminated against...           1.000000   \n",
       "2  The proactive steps that can be taken to ensur...           1.000000   \n",
       "3  The steps involved in the design stage equity ...           0.926667   \n",
       "4  The Equal Employment Opportunity Commission an...           1.000000   \n",
       "\n",
       "   context_recall  \n",
       "0        1.000000  \n",
       "1        1.000000  \n",
       "2        0.666667  \n",
       "3        1.000000  \n",
       "4        1.000000  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_result.to_pandas().head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
